{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load env ..\n",
      "load env completed\n",
      "policy\n",
      "compute_avg_return ... \n",
      " init buffer ... \n",
      "collected\n",
      "dataset\n",
      "train\n",
      "save\n",
      "root_dir /home/leiningc/project/data\n",
      "train_dir /home/leiningc/project/data/trainbubble\n",
      "End save para\n",
      "step = 21000: loss = 74198.46875\n",
      "save the agent\n",
      "step = 21000: Average Return = -189.89999389648438\n",
      "step = 22000: loss = 82865.6875\n",
      "save the agent\n",
      "step = 22000: Average Return = -196.1999969482422\n",
      "step = 23000: loss = 14106.4248046875\n",
      "save the agent\n",
      "step = 23000: Average Return = -192.6999969482422\n",
      "step = 24000: loss = 2565.498046875\n",
      "save the agent\n",
      "step = 24000: Average Return = -180.5\n",
      "step = 25000: loss = 3152.12646484375\n",
      "save the agent\n",
      "step = 25000: Average Return = -170.0\n",
      "step = 26000: loss = 774.3969116210938\n",
      "save the agent\n",
      "step = 26000: Average Return = -189.1999969482422\n",
      "step = 27000: loss = 13.934288024902344\n",
      "save the agent\n",
      "step = 27000: Average Return = -181.39999389648438\n",
      "step = 28000: loss = 1306.9122314453125\n",
      "save the agent\n",
      "step = 28000: Average Return = -200.39999389648438\n",
      "step = 29000: loss = 1898.8424072265625\n",
      "save the agent\n",
      "step = 29000: Average Return = -184.6999969482422\n",
      "step = 30000: loss = 1390.947998046875\n",
      "save the agent\n",
      "step = 30000: Average Return = -190.60000610351562\n",
      "step = 31000: loss = 1949.9228515625\n",
      "save the agent\n",
      "step = 31000: Average Return = -186.3000030517578\n",
      "step = 32000: loss = 1331.4267578125\n",
      "save the agent\n",
      "step = 32000: Average Return = -156.39999389648438\n",
      "step = 33000: loss = 1315.180908203125\n",
      "save the agent\n",
      "step = 33000: Average Return = -196.3000030517578\n",
      "step = 34000: loss = 2136.0341796875\n",
      "save the agent\n",
      "step = 34000: Average Return = -183.6999969482422\n",
      "step = 35000: loss = 2912.3466796875\n",
      "save the agent\n",
      "step = 35000: Average Return = -173.39999389648438\n",
      "step = 36000: loss = 1296.179443359375\n",
      "save the agent\n",
      "step = 36000: Average Return = -185.89999389648438\n",
      "step = 37000: loss = 696.2282104492188\n",
      "save the agent\n",
      "step = 37000: Average Return = -159.1999969482422\n",
      "step = 38000: loss = 632.4031372070312\n",
      "save the agent\n",
      "step = 38000: Average Return = -183.89999389648438\n",
      "step = 39000: loss = 1427.7100830078125\n",
      "save the agent\n",
      "step = 39000: Average Return = -170.39999389648438\n",
      "step = 40000: loss = 3190.908203125\n",
      "save the agent\n",
      "step = 40000: Average Return = -182.1999969482422\n",
      "step = 41000: loss = 1871.11572265625\n",
      "save the agent\n",
      "step = 41000: Average Return = -166.5\n",
      "step = 42000: loss = 1904.34375\n",
      "save the agent\n",
      "step = 42000: Average Return = -189.39999389648438\n",
      "step = 43000: loss = 786.8145751953125\n",
      "save the agent\n",
      "step = 43000: Average Return = -131.3000030517578\n",
      "step = 44000: loss = 1277.6690673828125\n",
      "save the agent\n",
      "step = 44000: Average Return = -192.10000610351562\n",
      "step = 45000: loss = 3141.953369140625\n",
      "save the agent\n",
      "step = 45000: Average Return = -187.0\n",
      "step = 46000: loss = 1946.789306640625\n",
      "save the agent\n",
      "step = 46000: Average Return = -199.39999389648438\n",
      "step = 47000: loss = 64.79296112060547\n",
      "save the agent\n",
      "step = 47000: Average Return = -172.0\n",
      "step = 48000: loss = 743.5849609375\n",
      "save the agent\n",
      "step = 48000: Average Return = -196.3000030517578\n",
      "step = 49000: loss = 1357.2254638671875\n",
      "save the agent\n",
      "step = 49000: Average Return = -173.10000610351562\n",
      "step = 50000: loss = 1442.955322265625\n",
      "save the agent\n",
      "step = 50000: Average Return = -154.1999969482422\n",
      "step = 51000: loss = 1294.928466796875\n",
      "save the agent\n",
      "step = 51000: Average Return = -194.39999389648438\n",
      "step = 52000: loss = 1375.0028076171875\n",
      "save the agent\n",
      "step = 52000: Average Return = -188.6999969482422\n",
      "step = 53000: loss = 693.1055297851562\n",
      "save the agent\n",
      "step = 53000: Average Return = -181.8000030517578\n",
      "step = 54000: loss = 1906.0733642578125\n",
      "save the agent\n",
      "step = 54000: Average Return = -200.8000030517578\n",
      "step = 55000: loss = 647.0155029296875\n",
      "save the agent\n",
      "step = 55000: Average Return = -186.0\n",
      "step = 56000: loss = 3057.834228515625\n",
      "save the agent\n",
      "step = 56000: Average Return = -174.10000610351562\n",
      "step = 57000: loss = 661.6190185546875\n",
      "save the agent\n",
      "step = 57000: Average Return = -205.60000610351562\n",
      "step = 58000: loss = 1278.0947265625\n",
      "save the agent\n",
      "step = 58000: Average Return = -188.1999969482422\n",
      "step = 59000: loss = 3653.525390625\n",
      "save the agent\n",
      "step = 59000: Average Return = -184.10000610351562\n",
      "step = 60000: loss = 3077.83056640625\n",
      "save the agent\n",
      "step = 60000: Average Return = -197.1999969482422\n",
      "step = 61000: loss = 618.033447265625\n",
      "save the agent\n",
      "step = 61000: Average Return = -173.10000610351562\n",
      "step = 62000: loss = 3075.860595703125\n",
      "save the agent\n",
      "step = 62000: Average Return = -190.89999389648438\n",
      "step = 63000: loss = 3096.990234375\n",
      "save the agent\n",
      "step = 63000: Average Return = -191.0\n",
      "step = 64000: loss = 1230.4686279296875\n",
      "save the agent\n",
      "step = 64000: Average Return = -168.1999969482422\n",
      "step = 65000: loss = 3612.257080078125\n",
      "save the agent\n",
      "step = 65000: Average Return = -176.39999389648438\n",
      "step = 66000: loss = 3361.78271484375\n",
      "save the agent\n",
      "step = 66000: Average Return = -160.3000030517578\n",
      "step = 67000: loss = 1846.431396484375\n",
      "save the agent\n",
      "step = 67000: Average Return = -193.1999969482422\n",
      "step = 68000: loss = 1216.03515625\n",
      "save the agent\n",
      "step = 68000: Average Return = -161.6999969482422\n",
      "step = 69000: loss = 3596.424072265625\n",
      "save the agent\n",
      "step = 69000: Average Return = -189.0\n",
      "step = 70000: loss = 1825.204345703125\n",
      "save the agent\n",
      "step = 70000: Average Return = -178.39999389648438\n",
      "step = 71000: loss = 1823.23828125\n",
      "save the agent\n",
      "step = 71000: Average Return = -183.3000030517578\n",
      "step = 72000: loss = 1774.106201171875\n",
      "save the agent\n",
      "step = 72000: Average Return = -163.8000030517578\n",
      "step = 73000: loss = 636.8378295898438\n",
      "save the agent\n",
      "step = 73000: Average Return = -188.8000030517578\n",
      "step = 74000: loss = 1227.542724609375\n",
      "save the agent\n",
      "step = 74000: Average Return = -188.60000610351562\n",
      "step = 75000: loss = 1220.57275390625\n",
      "save the agent\n",
      "step = 75000: Average Return = -170.1999969482422\n",
      "step = 76000: loss = 641.0701293945312\n",
      "save the agent\n",
      "step = 76000: Average Return = -182.5\n",
      "step = 77000: loss = 662.8070678710938\n",
      "save the agent\n",
      "step = 77000: Average Return = -148.6999969482422\n",
      "step = 78000: loss = 665.656494140625\n",
      "save the agent\n",
      "step = 78000: Average Return = -179.10000610351562\n",
      "step = 79000: loss = 1254.9312744140625\n",
      "save the agent\n",
      "step = 79000: Average Return = -175.60000610351562\n",
      "step = 80000: loss = 1815.4095458984375\n",
      "save the agent\n",
      "step = 80000: Average Return = -183.10000610351562\n",
      "step = 81000: loss = 1200.1300048828125\n",
      "save the agent\n",
      "step = 81000: Average Return = -171.6999969482422\n",
      "step = 82000: loss = 1802.51611328125\n",
      "save the agent\n",
      "step = 82000: Average Return = -192.10000610351562\n",
      "step = 83000: loss = 1271.0048828125\n",
      "save the agent\n",
      "step = 83000: Average Return = -174.8000030517578\n",
      "step = 84000: loss = 1333.668701171875\n",
      "save the agent\n",
      "step = 84000: Average Return = -194.1999969482422\n",
      "step = 85000: loss = 621.9180908203125\n",
      "save the agent\n",
      "step = 85000: Average Return = -164.39999389648438\n",
      "step = 86000: loss = 23.30133056640625\n",
      "save the agent\n",
      "step = 86000: Average Return = -186.39999389648438\n",
      "step = 87000: loss = 660.272705078125\n",
      "save the agent\n",
      "step = 87000: Average Return = -190.39999389648438\n",
      "step = 88000: loss = 2926.9150390625\n",
      "save the agent\n",
      "step = 88000: Average Return = -169.0\n",
      "step = 89000: loss = 1841.765625\n",
      "save the agent\n",
      "step = 89000: Average Return = -172.6999969482422\n",
      "step = 90000: loss = 1853.0928955078125\n",
      "save the agent\n",
      "step = 90000: Average Return = -184.8000030517578\n",
      "step = 91000: loss = 641.6856079101562\n",
      "save the agent\n",
      "step = 91000: Average Return = -172.5\n",
      "step = 92000: loss = 1265.3763427734375\n",
      "save the agent\n",
      "step = 92000: Average Return = -194.6999969482422\n",
      "step = 93000: loss = 611.805908203125\n",
      "save the agent\n",
      "step = 93000: Average Return = -172.60000610351562\n",
      "step = 94000: loss = 2909.11279296875\n",
      "save the agent\n",
      "step = 94000: Average Return = -191.0\n",
      "step = 95000: loss = 1268.2669677734375\n",
      "save the agent\n",
      "step = 95000: Average Return = -185.1999969482422\n",
      "step = 96000: loss = 1817.4793701171875\n",
      "save the agent\n",
      "step = 96000: Average Return = -164.10000610351562\n",
      "step = 97000: loss = 1773.578125\n",
      "save the agent\n",
      "step = 97000: Average Return = -180.39999389648438\n",
      "step = 98000: loss = 596.5491943359375\n",
      "save the agent\n",
      "step = 98000: Average Return = -189.5\n",
      "step = 99000: loss = 1317.467529296875\n",
      "save the agent\n",
      "step = 99000: Average Return = -146.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 100000: loss = 4063.693603515625\n",
      "save the agent\n",
      "step = 100000: Average Return = -167.3000030517578\n",
      "step = 101000: loss = 1742.9202880859375\n",
      "save the agent\n",
      "step = 101000: Average Return = -145.39999389648438\n",
      "step = 102000: loss = 2898.53466796875\n",
      "save the agent\n",
      "step = 102000: Average Return = -147.0\n",
      "step = 103000: loss = 1208.40771484375\n",
      "save the agent\n",
      "step = 103000: Average Return = -146.89999389648438\n",
      "step = 104000: loss = 1839.374267578125\n",
      "save the agent\n",
      "step = 104000: Average Return = -195.5\n",
      "step = 105000: loss = 1219.9755859375\n",
      "save the agent\n",
      "step = 105000: Average Return = -196.6999969482422\n",
      "step = 106000: loss = 1223.619140625\n",
      "save the agent\n",
      "step = 106000: Average Return = -158.1999969482422\n",
      "step = 107000: loss = 2963.131591796875\n",
      "save the agent\n",
      "step = 107000: Average Return = -193.39999389648438\n",
      "step = 108000: loss = 2211.13525390625\n",
      "save the agent\n",
      "step = 108000: Average Return = -182.1999969482422\n",
      "step = 109000: loss = 1229.3485107421875\n",
      "save the agent\n",
      "step = 109000: Average Return = -185.6999969482422\n",
      "step = 110000: loss = 1276.3681640625\n",
      "save the agent\n",
      "step = 110000: Average Return = -187.8000030517578\n",
      "step = 111000: loss = 2286.247802734375\n",
      "save the agent\n",
      "step = 111000: Average Return = -188.39999389648438\n",
      "step = 112000: loss = 3215.3369140625\n",
      "save the agent\n",
      "step = 112000: Average Return = -124.0\n",
      "step = 113000: loss = 1207.134033203125\n",
      "save the agent\n",
      "step = 113000: Average Return = -195.8000030517578\n",
      "step = 114000: loss = 1167.513916015625\n",
      "save the agent\n",
      "step = 114000: Average Return = -186.39999389648438\n",
      "step = 115000: loss = 1204.861083984375\n",
      "save the agent\n",
      "step = 115000: Average Return = -181.8000030517578\n",
      "step = 116000: loss = 1738.0125732421875\n",
      "save the agent\n",
      "step = 116000: Average Return = -183.89999389648438\n",
      "step = 117000: loss = 1694.32763671875\n",
      "save the agent\n",
      "step = 117000: Average Return = -188.89999389648438\n",
      "step = 118000: loss = 2359.026611328125\n",
      "save the agent\n",
      "step = 118000: Average Return = -191.39999389648438\n",
      "step = 119000: loss = 1181.234619140625\n",
      "save the agent\n",
      "step = 119000: Average Return = -180.39999389648438\n",
      "step = 120000: loss = 1451.62890625\n",
      "save the agent\n",
      "step = 120000: Average Return = -167.89999389648438\n",
      "step = 121000: loss = 1168.282470703125\n",
      "save the agent\n",
      "step = 121000: Average Return = -160.89999389648438\n",
      "step = 122000: loss = 2887.0673828125\n",
      "save the agent\n",
      "step = 122000: Average Return = -153.39999389648438\n",
      "step = 123000: loss = 609.9351196289062\n",
      "save the agent\n",
      "step = 123000: Average Return = -145.89999389648438\n",
      "step = 124000: loss = 1832.0960693359375\n",
      "save the agent\n",
      "step = 124000: Average Return = -153.5\n",
      "step = 125000: loss = 2251.965576171875\n",
      "save the agent\n",
      "step = 125000: Average Return = -130.10000610351562\n",
      "step = 126000: loss = 616.5264282226562\n",
      "save the agent\n",
      "step = 126000: Average Return = -192.60000610351562\n",
      "step = 127000: loss = 1180.9217529296875\n",
      "save the agent\n",
      "step = 127000: Average Return = -165.10000610351562\n",
      "step = 128000: loss = 60.841835021972656\n",
      "save the agent\n",
      "step = 128000: Average Return = -162.5\n",
      "step = 129000: loss = 1871.6533203125\n",
      "save the agent\n",
      "step = 129000: Average Return = -126.5\n",
      "step = 130000: loss = 1679.715087890625\n",
      "save the agent\n",
      "step = 130000: Average Return = -203.89999389648438\n",
      "step = 131000: loss = 1190.3424072265625\n",
      "save the agent\n",
      "step = 131000: Average Return = -173.89999389648438\n",
      "step = 132000: loss = 1752.02734375\n",
      "save the agent\n",
      "step = 132000: Average Return = -199.39999389648438\n",
      "step = 133000: loss = 2251.86474609375\n",
      "save the agent\n",
      "step = 133000: Average Return = -163.5\n",
      "step = 134000: loss = 3367.793212890625\n",
      "save the agent\n",
      "step = 134000: Average Return = -195.3000030517578\n",
      "step = 135000: loss = 643.7711791992188\n",
      "save the agent\n",
      "step = 135000: Average Return = -184.39999389648438\n",
      "step = 136000: loss = 2780.6259765625\n",
      "save the agent\n",
      "step = 136000: Average Return = -182.1999969482422\n",
      "step = 137000: loss = 1708.72705078125\n",
      "save the agent\n",
      "step = 137000: Average Return = -94.5\n",
      "step = 138000: loss = 1153.0577392578125\n",
      "save the agent\n",
      "step = 138000: Average Return = -176.89999389648438\n",
      "step = 139000: loss = 1135.4072265625\n",
      "save the agent\n",
      "step = 139000: Average Return = -121.80000305175781\n",
      "step = 140000: loss = 582.7967529296875\n",
      "save the agent\n",
      "step = 140000: Average Return = -88.9000015258789\n",
      "step = 141000: loss = 1159.809814453125\n",
      "save the agent\n",
      "step = 141000: Average Return = -200.0\n",
      "step = 142000: loss = 587.2521362304688\n",
      "save the agent\n",
      "step = 142000: Average Return = -194.8000030517578\n",
      "step = 143000: loss = 1654.357421875\n",
      "save the agent\n",
      "step = 143000: Average Return = -174.1999969482422\n",
      "step = 144000: loss = 2788.46826171875\n",
      "save the agent\n",
      "step = 144000: Average Return = -195.6999969482422\n",
      "step = 145000: loss = 1709.9168701171875\n",
      "save the agent\n",
      "step = 145000: Average Return = -199.60000610351562\n",
      "step = 146000: loss = 1213.328369140625\n",
      "save the agent\n",
      "step = 146000: Average Return = -188.1999969482422\n",
      "step = 147000: loss = 2205.86865234375\n",
      "save the agent\n",
      "step = 147000: Average Return = -193.39999389648438\n",
      "step = 148000: loss = 1128.1260986328125\n",
      "save the agent\n",
      "step = 148000: Average Return = -180.8000030517578\n",
      "step = 149000: loss = 1177.5147705078125\n",
      "save the agent\n",
      "step = 149000: Average Return = -198.5\n",
      "step = 150000: loss = 2811.66064453125\n",
      "save the agent\n",
      "step = 150000: Average Return = -190.3000030517578\n",
      "step = 151000: loss = 576.5965576171875\n",
      "save the agent\n",
      "step = 151000: Average Return = -185.0\n",
      "step = 152000: loss = 1677.620361328125\n",
      "save the agent\n",
      "step = 152000: Average Return = -191.89999389648438\n",
      "step = 153000: loss = 1716.45849609375\n",
      "save the agent\n",
      "step = 153000: Average Return = -203.39999389648438\n",
      "step = 154000: loss = 604.2256469726562\n",
      "save the agent\n",
      "step = 154000: Average Return = -192.39999389648438\n",
      "step = 155000: loss = 1654.28515625\n",
      "save the agent\n",
      "step = 155000: Average Return = -208.5\n",
      "step = 156000: loss = 1098.2811279296875\n",
      "save the agent\n",
      "step = 156000: Average Return = -176.3000030517578\n",
      "step = 157000: loss = 2761.5498046875\n",
      "save the agent\n",
      "step = 157000: Average Return = -186.39999389648438\n",
      "step = 158000: loss = 2813.177490234375\n",
      "save the agent\n",
      "step = 158000: Average Return = -165.60000610351562\n",
      "step = 159000: loss = 1181.464599609375\n",
      "save the agent\n",
      "step = 159000: Average Return = -184.3000030517578\n",
      "step = 160000: loss = 1682.251953125\n",
      "save the agent\n",
      "step = 160000: Average Return = -194.10000610351562\n",
      "step = 161000: loss = 3819.61865234375\n",
      "save the agent\n",
      "step = 161000: Average Return = -208.1999969482422\n",
      "step = 162000: loss = 1156.26806640625\n",
      "save the agent\n",
      "step = 162000: Average Return = -209.60000610351562\n",
      "step = 163000: loss = 1619.1519775390625\n",
      "save the agent\n",
      "step = 163000: Average Return = -192.5\n",
      "step = 164000: loss = 2766.16064453125\n",
      "save the agent\n",
      "step = 164000: Average Return = -180.89999389648438\n",
      "step = 165000: loss = 1617.3270263671875\n",
      "save the agent\n",
      "step = 165000: Average Return = -198.1999969482422\n",
      "step = 166000: loss = 2266.246337890625\n",
      "save the agent\n",
      "step = 166000: Average Return = -194.60000610351562\n",
      "step = 167000: loss = 1619.1805419921875\n",
      "save the agent\n",
      "step = 167000: Average Return = -176.10000610351562\n",
      "step = 168000: loss = 600.2855224609375\n",
      "save the agent\n",
      "step = 168000: Average Return = -197.6999969482422\n",
      "step = 169000: loss = 3259.0771484375\n",
      "save the agent\n",
      "step = 169000: Average Return = -202.89999389648438\n",
      "step = 170000: loss = 16.23711585998535\n",
      "save the agent\n",
      "step = 170000: Average Return = -200.60000610351562\n",
      "step = 171000: loss = 2658.447265625\n",
      "save the agent\n",
      "step = 171000: Average Return = -198.8000030517578\n",
      "step = 172000: loss = 581.5179443359375\n",
      "save the agent\n",
      "step = 172000: Average Return = -205.5\n",
      "step = 173000: loss = 2181.7724609375\n",
      "save the agent\n",
      "step = 173000: Average Return = -182.39999389648438\n",
      "step = 174000: loss = 2647.5673828125\n",
      "save the agent\n",
      "step = 174000: Average Return = -198.0\n",
      "step = 175000: loss = 3738.447998046875\n",
      "save the agent\n",
      "step = 175000: Average Return = -202.5\n",
      "step = 176000: loss = 595.0115966796875\n",
      "save the agent\n",
      "step = 176000: Average Return = -204.0\n",
      "step = 177000: loss = 4223.82177734375\n",
      "save the agent\n",
      "step = 177000: Average Return = -174.39999389648438\n",
      "step = 178000: loss = 1599.4146728515625\n",
      "save the agent\n",
      "step = 178000: Average Return = -196.60000610351562\n",
      "step = 179000: loss = 3703.27294921875\n",
      "save the agent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 179000: Average Return = -178.1999969482422\n",
      "step = 180000: loss = 3193.283203125\n",
      "save the agent\n",
      "step = 180000: Average Return = -146.8000030517578\n",
      "step = 181000: loss = 1158.21435546875\n",
      "save the agent\n",
      "step = 181000: Average Return = -197.60000610351562\n",
      "step = 182000: loss = 593.4240112304688\n",
      "save the agent\n",
      "step = 182000: Average Return = -196.8000030517578\n",
      "step = 183000: loss = 1617.49072265625\n",
      "save the agent\n",
      "step = 183000: Average Return = -174.3000030517578\n",
      "step = 184000: loss = 2111.273681640625\n",
      "save the agent\n",
      "step = 184000: Average Return = -192.89999389648438\n",
      "step = 185000: loss = 1603.5963134765625\n",
      "save the agent\n",
      "step = 185000: Average Return = -178.89999389648438\n",
      "step = 186000: loss = 2616.990966796875\n",
      "save the agent\n",
      "step = 186000: Average Return = -184.10000610351562\n",
      "step = 187000: loss = 1051.292236328125\n",
      "save the agent\n",
      "step = 187000: Average Return = -196.10000610351562\n",
      "step = 188000: loss = 2140.5146484375\n",
      "save the agent\n",
      "step = 188000: Average Return = -166.10000610351562\n",
      "step = 189000: loss = 2763.5537109375\n",
      "save the agent\n",
      "step = 189000: Average Return = -198.0\n",
      "step = 190000: loss = 577.8131713867188\n",
      "save the agent\n",
      "step = 190000: Average Return = -204.8000030517578\n",
      "step = 191000: loss = 3314.392333984375\n",
      "save the agent\n",
      "step = 191000: Average Return = -193.89999389648438\n",
      "step = 192000: loss = 2132.5126953125\n",
      "save the agent\n",
      "step = 192000: Average Return = -202.0\n",
      "step = 193000: loss = 4751.9775390625\n",
      "save the agent\n",
      "step = 193000: Average Return = -193.39999389648438\n",
      "step = 194000: loss = 2619.82958984375\n",
      "save the agent\n",
      "step = 194000: Average Return = -187.39999389648438\n",
      "step = 195000: loss = 1614.53466796875\n",
      "save the agent\n",
      "step = 195000: Average Return = -189.89999389648438\n",
      "step = 196000: loss = 1611.421875\n",
      "save the agent\n",
      "step = 196000: Average Return = -197.6999969482422\n",
      "step = 197000: loss = 2097.031005859375\n",
      "save the agent\n",
      "step = 197000: Average Return = -190.0\n",
      "step = 198000: loss = 1681.9605712890625\n",
      "save the agent\n",
      "step = 198000: Average Return = -178.5\n",
      "step = 199000: loss = 1579.44677734375\n",
      "save the agent\n",
      "step = 199000: Average Return = -199.1999969482422\n",
      "step = 200000: loss = 2102.59375\n",
      "save the agent\n",
      "step = 200000: Average Return = -195.60000610351562\n",
      "step = 201000: loss = 1563.995361328125\n",
      "save the agent\n",
      "step = 201000: Average Return = -199.5\n",
      "step = 202000: loss = 2595.6982421875\n",
      "save the agent\n",
      "step = 202000: Average Return = -181.60000610351562\n",
      "step = 203000: loss = 1068.2021484375\n",
      "save the agent\n",
      "step = 203000: Average Return = -154.39999389648438\n",
      "step = 204000: loss = 2081.652587890625\n",
      "save the agent\n",
      "step = 204000: Average Return = -180.6999969482422\n",
      "step = 205000: loss = 1131.6534423828125\n",
      "save the agent\n",
      "step = 205000: Average Return = -128.39999389648438\n",
      "step = 206000: loss = 549.23095703125\n",
      "save the agent\n",
      "step = 206000: Average Return = -169.89999389648438\n",
      "step = 207000: loss = 2097.79052734375\n",
      "save the agent\n",
      "step = 207000: Average Return = -108.9000015258789\n",
      "step = 208000: loss = 2085.014892578125\n",
      "save the agent\n",
      "step = 208000: Average Return = -172.89999389648438\n",
      "step = 209000: loss = 1034.1900634765625\n",
      "save the agent\n",
      "step = 209000: Average Return = -202.0\n",
      "step = 210000: loss = 3124.45166015625\n",
      "save the agent\n",
      "step = 210000: Average Return = -206.10000610351562\n",
      "step = 211000: loss = 2037.70703125\n",
      "save the agent\n",
      "step = 211000: Average Return = -176.0\n",
      "step = 212000: loss = 2560.85009765625\n",
      "save the agent\n",
      "step = 212000: Average Return = -186.3000030517578\n",
      "step = 213000: loss = 1529.774658203125\n",
      "save the agent\n",
      "step = 213000: Average Return = -204.89999389648438\n",
      "step = 214000: loss = 3614.03759765625\n",
      "save the agent\n",
      "step = 214000: Average Return = -186.8000030517578\n",
      "step = 215000: loss = 542.82177734375\n",
      "save the agent\n",
      "step = 215000: Average Return = -191.5\n",
      "step = 216000: loss = 2569.40234375\n",
      "save the agent\n",
      "step = 216000: Average Return = -185.39999389648438\n",
      "step = 217000: loss = 1048.4884033203125\n",
      "save the agent\n",
      "step = 217000: Average Return = -201.8000030517578\n",
      "step = 218000: loss = 1549.033203125\n",
      "save the agent\n",
      "step = 218000: Average Return = -190.10000610351562\n",
      "step = 219000: loss = 1040.922607421875\n",
      "save the agent\n",
      "step = 219000: Average Return = -187.0\n",
      "step = 220000: loss = 2024.9619140625\n",
      "save the agent\n",
      "step = 220000: Average Return = -194.1999969482422\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEKCAYAAADTgGjXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xd8Y1eZ+P/PkWVLLnKv4xmPx9NLJjPJZCa9k0IqJTCEXQIEAiGwEJYSCHW/G34LLJ0NbIBsAoT0hCSEdGZC+mR6L57u3qssWeX8/rjFsi3ZsmK5ZJ736+WX7SvJenwl3eec85x7rtJaI4QQQiTCMdkBCCGEmL4kiQghhEiYJBEhhBAJkyQihBAiYZJEhBBCJEySiBBCiIRJEhFCCJEwSSJCCCESJklECCFEwpyTHUCyFRYW6srKyskOQwghppVNmza1aK2LRrvfuz6JVFZWsnHjxskOQwghphWl1NF47ifDWUIIIRImSUQIIUTCJIkIIYRImCQRIYQQCZMkIoQQImGSRIQQQiRMkogQQoiESRIRQgiRMEkiQgghEiZJRAghRMIkiQghhEiYJBEhhBAJkyQihBAiYZJEhBBCJEySiBBCiIRJEhFCCJEwSSJCCCESJklECCFEwiYtiSilZiml1iml9iildimlvmhuz1dKvaCUOmB+z4t4zDeUUtVKqX1KqUsnK3YhhBCGyeyJBIF/11ovBk4HblFKLQFuA17SWs8HXjJ/x7xtLbAUuAy4UymVMimRCyGEACYxiWit67XWm82fu4E9QDlwDXCvebd7gWvNn68BHtBa+7XWh4FqYPXERi2EECLSlKiJKKUqgZXAW0CJ1roejEQDFJt3KweORzysxtwmhBBikkx6ElFKZQGPAl/SWneNdNco23SMv3mTUmqjUmpjc3PzeIQphBAiiklNIkqpVIwEcp/W+jFzc6NSqsy8vQxoMrfXALMiHj4TqIv2d7XWd2mtV2mtVxUVFSUneCGEEJM6O0sBfwD2aK1/GnHTk8AN5s83AE9EbF+rlHIppeYA84ENExWvEEKI4ZyT+NxnAf8K7FBKbTW3fRP4L+AhpdSNwDHgOgCt9S6l1EPAboyZXbdorUMTH7YQQgjLpCURrfWrRK9zAFwU4zF3AHckLSghhBBjMumFdSGEENOXJBEhhBAJkyQihBAiYZJEhBBCJEySiBBCiIRJEhFCCJEwSSJCCCESJklECCFEwiSJCCGESJgkESGEEAmTJCKEECJhkkSEEEIkTJKIEEKIhEkSEUIIkTBJIkIIIRImSUQIIUTCJIkIIYRImCQRIYQQCZMkIoQQImGSRIQQQiRMkogQQoiESRIRQgiRMEkiQgghEiZJRAghRMIkiQghhEiYJBEhhBAJkyQihBAiYZJEhBBCJEySiBBCiIRJEhFCCJEwSSJCCCESJklECCFEwiSJCCGESJgkESGEEAmTJCKEECJhkkSEEEIkTJKIEEKIhEkSEUIIkbBJTSJKqbuVUk1KqZ0R2/KVUi8opQ6Y3/MibvuGUqpaKbVPKXXp5EQthBDCMtk9kXuAy4Zsuw14SWs9H3jJ/B2l1BJgLbDUfMydSqmUiQtVCCHEUJOaRLTW/wTahmy+BrjX/Ple4NqI7Q9orf1a68NANbB6QgIVQggR1WT3RKIp0VrXA5jfi83t5cDxiPvVmNuGUUrdpJTaqJTa2NzcnNRghRDiRDYVk0gsKso2He2OWuu7tNartNarioqKkhyWEEKcuJyj3UEpVQR8GqiMvL/W+pNJiqlRKVWmta5XSpUBTeb2GmBWxP1mAnVJikEIIUQc4umJPAHkAC8CT0d8JcuTwA3mzzeYz29tX6uUciml5gDzgQ1JjEMIIcQoRu2JABla668n48mVUvcD5wOFSqka4LvAfwEPKaVuBI4B1wForXcppR4CdgNB4BatdSgZcQkhhIhPPEnkb0qp92qt/z7eT661/kiMmy6Kcf87gDvGOw4hhBCJiWc464sYiaRPKdWllOpWSnUlOzAhhBBT34g9EaWUApZqrY9NUDxCCCGmkRF7IlprDTw+QbEIIYSYZuIZznpTKXVa0iMRQggx7cRTWL8A+IxS6ijQi3HSn9ZaL09qZEIIIaa8eJLI5UmPQgghxLQUTxKJurSIEEIIEU8SeRojkSjADcwB9mEsyS6EEOIENmoS0VqfFPm7UuoU4DNJi0gIIcS0MeZVfLXWmwGZrSWEECKuVXy/HPGrAzgFkIt0CCGEiKsm4on4OYhRI3k0OeEIIYSYTuJJIru11g9HblBKXQc8HOP+QgghThDx1ES+Eec2IYQQJ5iYPRGl1OXAe4FypdQvI27KxhjWEkIIcYIbaTirDtgIXA1sitjeDdyazKCEEEJMDzGTiNZ6G7BNKfUX834VWut9ExaZEEKIKS+emshlwFbgWQCl1Aql1JNJjUoIIcS0EE8S+R6wGugA0FpvBSqTF5IQQojpIp4kEtRadyY9EiGEENNOPOeJ7FRKXQ+kKKXmA/8GvJ7csIQQQkwH8fREvoCxYq8f+AvQBXwpmUEJIYSYHuJZxdcL3G5+AaCUmg0cTWJcQgghpoEReyJKqTOUUh9UShWbvy83p/y+OiHRCSGEmNJiJhGl1I+Bu4EPAE8rpb4LvAC8BcyfmPCEEEJMZSMNZ10BrNRa+5RSeRhnsC/XWh+YmNCEEEJMdSMNZ/VprX0AWut2YJ8kECGEEJFG6onMHXJmemXk71rrq5MXlhBCiOlgpCRyzZDff5LMQIQQQkw/Iy3A+PJEBiKEEGL6iedkQyGEECIqSSJCCCESFncSUUplJjMQIYQQ08+oSUQpdaZSajewx/z9ZKXUnUmPTAghxJQXT0/kZ8ClQCvYVzw8N5lBCSGEmB7iGs7SWh8fsimUhFiEEEJMM/FcT+S4UupMQCul0jCuJ7InuWEJIYSYDuLpiXwWuAUoB2qAFebvk0IpdZlSap9SqlopddtkxSGEECK+64m0AB+dgFhGpZRKAf4HeA9GQntbKfWk1nr35EYmhBAnplGTiFLql1E2dwIbtdZPjH9II1oNVGutDwEopR7AWJ5FkogQQkyCeIaz3BhDWAfMr+VAPnCjUurnSYwtmnIgsshfY24TQggxCeIprM8DLtRaBwGUUr8BnscYUtqRxNiiUVG26WF3Uuom4CaAioqKZMckhBAnrHh6IuVA5NnqmcAMrXUI8CclqthqgFkRv8/EuFjWIFrru7TWq7TWq4qKiiYsOCGEONHE0xP5EbBVKbUeoydwLvADcxmUF5MYWzRvA/OVUnOAWmAtcP0ExyCEEMIUz+ysPyil/o5R1FbAN7XWVuv/q8kMLkosQaXU54HngBTgbq31romMQQghxIB4eiIAPqAeo8g+Tyk1T2v9z+SFFZvW+u/A3yfjuYUQQgwWzxTfTwFfxKg/bAVOB94ALkxuaEIIIaa6eArrXwROA45qrS8AVgLNSY1KCCHEtBBPEvFprX0ASimX1novsDC5YQkhhJgO4qmJ1CilcoG/Ai8opdqJMq1WCCHEiSee2VnvM3/8nlJqHZADPJvUqIQQQkwLIyYRpZQD2K61XgagtX55QqISQggxLYxYE9Fah4FtSilZO0QIIcQw8dREyoBdSqkNQK+1UWt9ddKiEkIIMS3Ek0S+n/QohBBCTEvxFNZfVkrNBuZrrV9USmVgLDkihBDiBDfqeSJKqU8DjwD/a24qx5juK4QQ4gQXz8mGtwBnAV0AWusDQHEygxJCCDE9xJNE/FrrfusXpZSTKBeCEkIIceKJJ4m8rJT6JpCulHoP8DDwVHLDEkIIMR3Ek0Ruw1hwcQfwGYxl2L+VzKCEEEJMD/FM8b0G+KPW+nfJDkYIIcT0Ek9P5Gpgv1LqT0qpK8yaiBBCCDF6EtFafwKYh1ELuR44qJT6fbIDE0IIMfXF1avQWgeUUs9gzMpKxxji+lQyAxNCCDH1xXOy4WVKqXuAauCDwO8x1tMSQghxgounJ/Jx4AHgM1prf3LDEUIIMZ3Es3bW2sjflVJnAddrrW9JWlRCCCGmhbhqIkqpFRhF9Q8Bh4HHkhmUEEKI6SFmElFKLQDWAh8BWoEHAaW1vmCCYhNCCDHFjdQT2Qu8Alylta4GUErdOiFRCSGEmBZGmp31AaABWKeU+p1S6iJATUxYQgghpoOYSURr/bjW+sPAImA9cCtQopT6jVLqkgmKTwghxBQWzxnrvVrr+7TWVwIzga0YizIKIYQ4wcWzdpZNa92mtf5frfWFyQpICCHE9DGmJCKEEEJEkiQihBAiYZJEhBBCJEySiBBCiIRJEhFCCJEwSSJCCCESJklEiBOMLxCa7BDEu4gkESFOIEdaeln23efYVdc52aGIdwlJIkKcQPY3dhMMa2ra+yY7FPEuMSlJRCl1nVJql1IqrJRaNeS2byilqpVS+5RSl0ZsP1UptcO87ZdKKVkMUohRbDrazo6agV5HU7dxcdL+YHiyQhLvMpPVE9kJvB/4Z+RGpdQSjGuYLAUuA+5USqWYN/8GuAmYb35dNmHRCjFN/efTu/nx8/vs35u6fAD4JYmIcTIpSURrvUdrvS/KTdcAD2it/Vrrw0A1sFopVQZka63f0Fpr4I/AtRMYshDTUq8/SK8/aP8uPREx3uK6PO4EKgfejPi9xtwWMH8eul0IMYK+QAinY6Ct2Gj3RGSGlhgfSUsiSqkXgdIoN92utX4i1sOibNMjbI/13DdhDH1RUVExSqRCvHv5AmFSHQMJQ3oiYrwlLYlorS9O4GE1wKyI32cCdeb2mVG2x3ruu4C7AFatWhUz2QjxbufrD+F0DLTBrCQiNRExXqbaFN8ngbVKKZdSag5GAX2D1roe6FZKnW7OyvoYEKs3I4Qw+YIh+syTC4OhMC090hMR42uypvi+TylVA5wBPK2Ueg5Aa70LeAjYDTwL3KK1tvriNwO/xyi2HwSemfDAhZhGgqEwgZCmr9/4CLX29qPNfrnURKaW9t5+bv7zJjq8/ZMdyphNSmFda/048HiM2+4A7oiyfSOwLMmhCfGu4TN7G/5gmFBY09Tlt2+TnsjUsrWmg2d2NrB2dQXnLSia7HDGZKoNZwkhxonVAwFjvSxrZhZITWSqsaZhR07Hni4kiQjxLhW50GJfIGQX1dNSHNITmWJ6fEby6JEkIoSYKgYlkf4QTd1GT6Qs1y09kSmmR3oiQoipxhcIR/wcorHLT0FmGplpTkkiU4wkESHElNM3ZDirudtHcbabNKdDZmdNMQPDWdPvdZEkIsQ4+OMbR3h2Z/1khzHI0OGstt5+8jNTcTmlJjLVSE9EiBPc3a8e5uGNNaPf8R16YmstB5t74rrv0J5Ity+Ix5Vq9kQkiUwl3ZJEhDix9fhDePuTOxQRDmv+/aFt3PfmsbjuP7Qn0u0L4nE7cTlTpCcyBXR6A3z/qV34AiE7ecjsLJEUx9u89nIVYmrq9Qfx9o/PASAYCnPD3Rt4/WDLoO0dfQGCYU1fIL7nGTrFt9sXwOM2hrOkJjL5Xj/Ywv+9doRtxzvsmkjvOL2HJpIkkWngc/dt5gdP75nsMEQMobCmLxCid5x6IjXtfby8v5m3D7cP2m41JOLt8UTOzur1B+ntD5Gd7jRqIiHpiUy2Ll8AgHZvwO6BTMfC+lS7noiIoqnbR25G6mSHIWKwWo9945REjrT2Dvq7lubusSWRyJqI9ViP26yJBCSJTLauPuP1bff20+2TmohIoh5f0H6TianHa7Yex2so4libF2DYa271ROJNVpHDWY1dVhKRnshUYfVE2nr77ffOdEwi0hOZ4kJhTW9/aFoW3E4U1mszXoX1Iy1GEhl6QBnoicT3XugLhEhzOtBa22erZ7ud0hOZIrr6BpLIdF72RJLIFDed54+fKKzXpj8YJhAKk5ryzjr4R63hLP/QnoixTHhfnAnAHwjjdjrQDFyMyiisp0hPZAroMhNHQ6ePYFjjdCh6/UG01hiXTZoeZDhrirMLbjKcNWVFHuzHozdy1BrOitET6Yu3J9IfIj0thfTUlIgkYvREQmFNUBLJpLJ6Isfbjde72OMirAdPiJgOJIlMcd3muGlPv9FCEePjWKuXDYfbAHituoWvPbKNnbWd+IOhMR9cI4cgEi2ua6158O1jNHX77JrI8J7IGGdnBUOkp6aQnpZiP9aa4gucUL2RqfjZsWoi1utdkuMGoNsfmLSYEiFJZIqzeiBaj9+Y+2Rr7vYTDk/uh/rX6w5w0582mgfv4zy0sYYrf/UqC7/1LBf/9OUx/a3IgnqixfVXq1v4+qM7+Poj2+0TAYeOj4+1sN7XH8KdavRErGOo1RMBTpi6yIbDbSz+zrNT7lwra3ZWh9dIGqXZRhLpnWbTfCWJTHGRQxrTseg2VKc3wNk//AdPba+b1Dhaevrp8AZo7e3ncEsvp1Xm8f2rl3LugiKOtHoHzWwaTeTcfm+CB4C7Xz0MwLp9zQBUFmTE7In0xRmbLxjGnZqCOzXF3madsQ4nTk9kZ20nvkCYmva+yQ5lEKsnYimxk8j0+pxLEpniIqd5vhum+dZ0ePEHwxxojG/9p2Rp6zWK1Ieaeznc0svSGTnccGYllywpAaCzL/4hhcE1kbG/Roeae1i3r5nLlpba25bOyBnUaAiHNa09/aQ4FMGwjmvZEl9/CHeqg3QzibicDlzOlBOuJ2Jd0XEsr+lE6OqLnkSmW2NRksgUF1lQH+3N1R8M85Pn99HpnVoflkhWgbe+0zfKPZOrw2skkbcOtdLjDzKnMBOAnHTjpM7Ek8jYeyL3vH6EtBQH/+/aZayek09aioN5xVn4AmG7PmMteTIj1zjQxDOkFVkTAaMeAkTURKbXsEmirCQy9KA9GXr9Qf70xhECoTC9/SF7CAugNMdl32c6kSSSZLvrunhkU+Kru/ZEFNlGe3NtPNLGr/5Rzbp9TQk/X7I1mye9RV7vezK0m4n2xT2NAO8oifS8gyTS4e3n4Y01XL1iBkUeFz9430n8Yu0KPG5j9r01Pm4NZc3KyzCeJ471syJrImCcIwLYPZHpNgsoUQ1TqCfy9I56vv3ELt442ArA7IIM+7YSj/RERBT/99phvvXXHTFvP9LSO2JyGMtw1r7GbgBazaGaqai5x+qJTN74dCis7fHobTWdQJQkEtGb+/+e2cPr1S3EEvn6jbWwft9bx+gLhPjUOXMAmFecxeUnldlJxJqp02L24CryjYPOWHoiVk3E+psn2uws62z9oTWIyVBr1mV213cBQ5JIjhTWRRQNXT58gXDUsfJwWHPVr1/lzvXVMR/fPYbhrH0NRhJp603eLJT/WVfNX7fUAgMn141Fk9kqtD7Yk6GzL0DkjM80p4MZuekA9hplVqs1GApz1z8P8dT22Bec6vWHyM9MA8A7hlakPxjintePcM78QhaVZg+6LdM1uCdiJd9ZZhKJp8fT1x/GlZpCeprxMbeGs06kmojWOmI4a/Jb+LUdRhLZYycRo/HiUFCYJcNZIgpr7L+1Z3jvoLnHT7cvOGKRuccftIcjekZpSVk9kbbe5LS4wmHNneuqeXSzMTx385838fVHtw+7X38wHHN2k1UT6fEH7XNgJlq7WQ+ZlW8kjsqCDFIcxhnCQ4ez2r1GwhlpemiPP0iReQDwjmFW15Nb62ju9vPpc6qG3WYlEavhsKfeeG3tnsiQ56nr6BtWC/MHzJrIsJ7IiTM7q9sftBPuVBjOsnsidUYSsXrAmS4nWUNe8+lCkkiSNZhJxDpwRbKmHB4fYephty9AmdXNHaH1qbVmf5J7IodaeuntD9ktuz31Xeys7Rx2v9se286n/7gx6t+wzrqG+OsiX39kO09srU0gYkNTt481P3iRrcc7AGg3h/tOrcgDBj7IMNBa7zAPOK3mvoyMe6hef5CCrDQcKv4pvlpr/vDqYRaVejhnfuGw2z0RB5QfPbuX3758kHMXFNnvhaE9kY/dvYH/enbw5QL6AubsrDSn+b8NHs7yjyHhJaq2o29Sz89oiniPTYXhrDpzGNe6OqXVKPC4nKQ4FBlpKdITEQO6fQPXCYhWp7C6tsfbvMPOqL3t0e385a1j9PiD5GemkZbiGLEmUtvRZyeZtnGuiYTCmlBYs6PWOAg3mScLNnX7qW3vGxb7xiPt7IiSXKzHWjNSGjpHP7iEwppHN9fw9AjDSaPZfryTxi4/680JB1ZR/dTKfADmFGbZ901xKDxu58DieGYPcqQk0uMPkulykpHmjLuw/sqBFvY2dHPj2XOirpNk9URauv3cuf4gVy4v4w83rLJnWkUufaK15lib1164ESAQChMM60E9kexhs7OS3xO58Z63+dbjO5P+PLFY7zGHin92VlOXjyXfeZZNR9tHv/MYhMOa+g4jqVnn2uZlppGTnmq/3pku57S7MJUkkQQdbO4ZdSmFyJZ2W5ThLKtr2+MPmsMmGq013v4gD208zjM76+n2BclyO8lyOwfN1BpqvzmUVZLtGvfC+vt/8zrfe3IX280idIc3QF1nH0FzheHIsea+/hDH2710eAODWn4d3n57NdmTZuYA8RXXW3v8BMOawy299rbjbV6+8vC2uFu4VqvP6jVZvcI1c/JZUJI1rCeQk55qD320mPuypccf8/Xu7Q+S5XKSkZYS93kiD7x9jMIsF1evmBH1dmtow3pd37OkhNQUBxlmryJyOKvbH6Q/GKaxe+D9Zg0nGrOzJqcmEgiFqW7qidmgmAjWZ3B2QWbcSeRAUw/e/hC76sY37uYeP/2hMO7UgcNutttJfmYaWWYvMcvlnHYXppIkkoC3DrVy0U9e5o1DrSPeL/JciGi9g9qOgZbj0dZeLvv5K/zipQPsqusirI2LE/X4jANUpitl2CKMDZ0++0C8r8E4UJ5eVWAP14yHQCjMztpOHt503J6WCLCztivi/xhIBkZyNX4+bq4J1NDpY/UdL/Hwxhp8gTDLy40kMnQ4a93eJnvJcou1D4+2egmZzbcXdjfyyKYabrx3oz1LKRTWMQ/yh5qNBGQdzKxzRMpy3Dx/63mcNW9wEsnNGEgirWai8gfDwxZEtPT6Q2S6UsxW5OgHgFBY81p1KxcuKrLrE0NZScSqc1mF/wyzJxLZ47FmbjVFTFawkow7LfI8kYmtiRxt9RIMa2o7+iat/mVN751XnBV3TcR6zzWM87lM1ufk1NnGMKpDQWaak6rCTGaaU7czXTKcdUL4+w5jaGVHzcgtlcgkYvUOtNb87IX97KztpKa9zx5qePVAC/sau/nb9nq2mWP3te19tHn78bidZLlSh7VQvvLwNm59cCtgtFjLctzMLsikoy9gH3Cj2VnbGbPFvO14B1+4f4t9ktvxNuPg7QuE2dvQbdcPrKEtGJxErJaz8Vhj+/aaDvpDYe7bcAyAmfnp5GWkDto/jV0+PnHP2/zBXP5j6D7sD4WpM5/nUEsPaSkOdtR08IO/G3WAT/9xI1952Cjyf/2R7XzZ3C8w0BNp7PLT1OWj3RvA6VD2gXqoyJ5IZPKPNaTVaw5npaemxLXC7p76Ljr7Apw5d3gtxGINb1iTLqwkMjCcFWLT0TbqO/vsuHr8QXv41OpluJ2OYVN804bURDq8/fa+HU/WfgfYP4ErFHR6A3ZPrLHLR7bbSWm22156HYy6WKyTchvMhtl4zyC0Rh5WVxYARs/Q4VD86vqV/OgDywEjqUy3FbsliYyR1poXdhsnqO2LOGBGY7Vk8jJS7WL33oZufvHSAe5+9TC17X2sqjRaJdaMp+qmHp7d2QAY46Yd3gAedyoe1/DhrMMtvVQ3Ga3sQ809zC3KoiAzDa2jF/KtmK7+9avcv+F41Nuf29XAU9vqONBkfOiPthq9CeuAe9GiYgB7aAsYdADa39iDOdHJ7onsNQv+VnIs9rgpzUnnWJuXn72wn0PNPbxsrhl1oNEYJrzqV6/y5zeP2h9oMAr7xv/ay+IZ2bz3pDKe29VAty/Ay/ub2XzMGMN+/VALL+9vth93sLmHecVG3WNHbScd3n7yMtNiXrNh0HBWxDBkS5QkEgyF8QfDZKY5zVbk6D2R18xzTs6cWxDzPmlOB2lOB7UdfaQ4FCUeY/aX1ejw9of41L0b+cWLBwbFaPXurJ6ItRQ8RDtj3Ug033tyFx+7e8OoccfrWKuX9t7+QUnEmn4er5f3N49p/bJI1975Gj99YT9g7I/SHDfZ6Uady+qt3nzfJr780Naoj7d6L0N7xe+U1dhaPceoxWWnG5+pjDSn3TgozHINGqbVWvPUtrqE98VEkCQyRrvquqjr9OF0qFHXf6rv9FGQmUZpTrrdon1+l5GAXjvYQm1HH/OLPRRmpXGk1UuaeTGjjUfbKcl22X8ny2XVRAZaKMFQmIYuHy09frz9QY60epldkGGfrxBrSOuVA82E9cABfihrWWprHrtVi7jlgnk4FFx+krG+087aTpTCPtDVtHs53ualuqmb+cUePG6nfZ2EoQeQIo+L0mwXrxxo4RcvHeAHf9/D+v1G0bu6qYe6Th87ajt55UAz9V0+rGP9YfOgdKi5l7mFmZw7v4imbj9/evMoobDmeJsXb3+Q2vY+Wnv7ae7209bbT7s3wFXLZ6CUkUTaevvJG+Ga9YN7In5SU4wAmqPUYKykYRfW4/iwv3awlfnFWRRHLHkRjZW4S7PdOM33RmqKg9QURUuPn3ZvgCOtvYMOOlYSsWsizhQ7eVj/89CayO76Lg4294zbgerj92zgtse2c7Cpl2KPi8y0lEE91NFUN/Vww90beHzL2Gfk+YMhDrf02qMEDV1+SrLdZLtTCYY13v4QWmt213XZs/VCYT3ofCer8ZfoqgrHWr1Rh6TqOvrIdjtZVOoBBiY6RCrLcVPXOTBZZU99N1+4fwv3vXUsoVgmgiSRMXp+dyMOBVedPIMDTd0jLmne0NlHaY6bgsw0ezjruV0NOJTRVfb2hyjPS7dPIDt/YZE9c+mq5QMFV6Mm4hzUym3s9ttDVttrOunsC1BZkEmBmURiFddfOWC0gmN9QIYmkSOtvXhcTj57XhX//NoFrJyVh9OhaPcGKMxyUZ6bTm1HHzf/eTPv/83r7KjtZH5JFhX5GRE9kS6WlA2cTFfscVFmDs8sK8/mpb1NrNvbjEP4HnrgAAAgAElEQVQZF+jZbM6KOdDYQ0Onj5l56XhcTg6bZ/c3dPmoKsrkLLMg/pv1BwEImrUG6yXZ29DFITPxLJ+ZQ1VhJjtrO2n3BsjNSIv5umWnp9JpTnRo7elnbpHRi4k2nNVjDl9luVKMwnoc65u9fbhtxF6IJdNltE6t9bIs6akpdnI/3jZ4Cq1VF7FqRelpKayek8+vPrKS08zZaE6HwqGMnkgwFOZIixetB3qdQzV1+eK+xorWRjJft6+Z7TUdzCvOYkGph70NXTEf4wuEBg2vWmdzJ7JIp/X/HzGvDlnb7mVGTrp9/k+XL0C7N0CXL2g3NP7t/i186H/fsP/H+s7ET4j1BUJc8ctX+OU/Dgy7rba9j/K8DHIzUslMS4meRHLT8QXC9vLwVi//uV0NY45lokgSGaN/7G3k1Nl5nF6Vjy8Q5q3DbVz6s39G/ZDUd/ooy3GTn5lGW28/x9u87K7v4vo1FfZ9ynPT7bnia6oK7JlC5y8stsewjZqIc9AU38ghpFcOGEM3swsyyDOTiNXzeXZnPT9/0ejah8OaV82hlIYYScQ6kFhDUEdavVQWZqKUYmZeBg6Hss+sLck2ksiWo8aU3uZuP41dfuYXe5iVl8GxNmNJ9cMtvVy0uJjZBRmkOR3kpKfymXOr+O2/nMIfbjgNp0PRFwhx+UllaI09nfdIay9HWr2U5aQzpyiTQy299sGzqiiL8tx0qgoz6fYF7Vb2P/YOrBu2r6HbLqpXFWWysiKPt4+009ztH7EnkpueRn8ojC8QprW3n7nFWaQ4jNb/szvrB9XCrBbn0Cm+x9u8fP4vmwdNRgDYeLSNvkBoWDE/miyXEWO5mXAtGWlO+/+q7+yjvtNHpjkcYr2u1nku7lQHKQ7FVSfPwGGOMyqljOusB8Mcb++zh7UOtww/aHd4+znvx+v585tHR40XjOnTgZCxyvCBJmMYcWGJh30N3TEnPnzl4W18/P/etn/fZ36WDkWJJ9K6fbEnYtR3+jje5qWlp58FpR6yI04ijZzpt6e+i5f3N7PlWAf3vmH8j1ZPpLMvMObe2YbDbXT7g/bJhJFqO/ooz3WjlGJhqYfyvPRh97HOA7L/D/P13HikzZ7kMdVIEhmDtt5+dtV1ce78IuaXGF3S7z25i32N3bxqtvAjx3IbzPHY/Mw02nr6ed6spdx0zlz7wDAzLyKJzMnng6fOZFl5Nisqcqk0l0TwuJ14hkzxrW2PTCLGc1cWDu6J+AIhvv3ELu5cd5BgKMyuui7aevvJTEuhMaKovflYOz96di+d3gCdfQEcKqIn0tI7aH0fwB5qK812MyPXTZ35t64xp6suLPUwKz+dmvY+DjT2ENawqDSbK04qY+mMbJRSzC7I5LJlZZRku7ly+QzSUhzccEYlAP8wz+cIa9hR00FZjps5hZkcbum1x9mriox9c7aZdK9bNQswZniBcfLW3oZuDjYbRfiZeRlccVKZfRCxhv2iiTxrvbXHT1GWi8KsNA419/L5v2xh7V1v2PWdnogkkukypvgGQ2G++MAW/ra9no/87k1++vw++2+v29tEWoojziRi9USGJpEU+6Q1Yx91Mis/gyyXk8YuH09uq+PWB7eS7Xba76GhXM4U+oNhDjYNHKgPNvcOu9+bh4ykt+nYwESKcFjHnLk0tIc7tyiLhaUe2r2BqD05fzDEP/Y2seVYu728vTX8GXmwH6qvP8SN97zNnesODtoeOW3car0vKvUM9ET6ghyJ+LtPbqujx280Qn7y/D6OtXpp7e233/NjHdJab9b2IvcrGMPPh1p67Ykpd3/8NL539dJhjx9IIsb/0RDxOluLhUbSWk/6Bd4kiYzB6wdb0BrOml/IfLNQaxXX9zV0s7ehixvu3sB/P7ePHn+QDm+Aspx0CjLT6PYHeWlPI1VFmVQUZNjDGeW56Vy5fAYfP7OSxWXZrKkq4G9fOIcsl5NK8w2X5UolM82JLxDmSEsvDZ0+u0iXnppiT12tyB/oibT39vPo5hqau4256UdavfzT7LFcsbzMPmGwtqOPT927kTvXH+Rl8/bTKvNp6TFm7NS0ewed0Q1QZK42WpztpjzX+LBVFWXy39edzC/WruCixcVU5GfgD4Z5pdr4mwtLPXz10oU8dvOZw/br965eyqM3n8nymTnGMEswzIISY/+GNZSaSaS2o4/ddV0ohX1wvHRpKSkOxQdPnUm220lDl1GHWlGRy96GLt463EZVUSYpDsXZ8wvtJDvScJZ1wGnp8dPlC1KQmUaRx8VLe5sIhjUpDsXH7t7AU9vq7J5Ilssojvb2h/jN+oNsPtbBDz9wEu9fWc4v/1Ft90he2tvEmqp8e/bVSKz7DE0i6Wkpg9b+2t/UTZHHRXG2i7qOPr79153ML87imS+dS0GWi2hcTgf+YIhqMyl73AO9m0hvHDQaKFajAuCbj+/g3B+vi1pXsw661rj/3KIsls/MNf5WlCnxm4604+0PEQhpqs0Dr7XEy/E2L/5g9J7AsTYvYQ3bajoGbY9Mbs+YE1QWlnrsoaPOPqOO5FDGVO6nthkXR/vF2pV4+0P83+vG7MCTzZhHGtLyRrlktVXbq+v0DaqLHG7ppT8YZrE5rJubkRZ1dmBZjvFaD0wz9jMjx83MvHR7wk1dRx8bjxiXdv7xc/u4+KcvD4pjb0PXhBbiJYmMwWvVLXjcTpaX5+Bxp9q9ibyMVPY1drPFbK398Y2jfPXhbQCsmp1HfpZxwHrzUCtnmdM6P3VOFV+5ZAG5GaksLPXwvauX2us3WSrN1pDHPNkQ4Opfv8qtD26ltqOP/Mw0qooy0dpowbhTU0hNcZDtdtLU7eN/Xz5Eofnc+xu72XikjfnFWSydkUMwrGnt7eeW+zbbb/aHNxozti5bZhTPn9/VQFgPLBJnGdoTgYGT4a5ZUU5qioOZZu/qwbeP43I6qCzIQCkVdUZUTnoqJ83MwZ2aYteHrjhpBk5zf5Rluzm9qgCt4e7XDlOem25PWz1rXiFbvvMeFpR47GQ3pzCThSUedtYaxdOPmT2c1BQHV508w37NYrGSiNUSzs9KoyjLRX8wTE56Kk98/mxmF2Twhfu38G/3bwGMqZmZaU76g2F+98oh3rOkhA+fVsEd7zuJ2QUZfP3R7eys7eRQc689w2001kFm6LCHNdvK2pVaQ1GWi9JsN//c30JnX4DPnDd32DBYJGs462BTD0UeF8tm5NjDR/3BMF9+cCtbj3fwmpn8DpmF941H2njg7eP2/zmUVZO49T0LWFmRy0kzc1g5K5eSbFfUVQciZ9Htru+i2xcwJ5xkEdZGkToa67XZVdc16AJd9Z0+e/bZpqPtFGa5KMxy2TOhusye6My8DBaXZuMPhinNdnPO/EIqCzLsxUVPnmUlkeg9kdqOPtbc8RIfu3uDPXR8vM3LoeZeVpu1p8ielFXnWVyWPfyPRSjyuHA61EBPpMuoq563oIiNR9vRWvPfz+1j7V1v8syOen7/ymEOtfTajco/vXGEy37+Ct99YteIzzOeJImMYP2+Ji752cscbulFa80rB1o4o6rAnimzYlYui0o9XLuynP2N3Ww91kGWuQbOMzsb+My5VaypKrBbv2ENZ80zeiALSz18/sL5MaeZgjEEpBQUelz2WkpdviAbj7ZR3djDjFy33e2OHHLKz0zj8c21HGvz8t2rlqKU0VPacryDUyry7CuobT3ewdbjHXz10oUUeVx2veQS8wp71oyQOYWDh7OKzZ5ISbaLpTNySE1RgyYCACwty8bjctLQ6ePyZaX2PhvNPLOIvaIi1+6Jleakc3pVAV+4cB6BkKaqKGvQY6xW5qAkYraEF5Rk8aFVM+37vm9luRl77JlRVhKxWuYFmS67DnTugiLmFGby2M1n2heROmteAZWFGfaJgF2+IGtPM4bX0tNS+OEHllPb0ccHf/s6ABcuKolrX9hJJEpPBGC+WasB4z1Sku2mLxAiNUVFXY8rkstMItXNPcwtyqSqKNM+6L19pI3HttRyy32bqW7qYcWsXMLaqJN96687mZHj5uqTZ/Dg28eHDVFZB93zFxbx+OfOIifdOBfi8mVlrN/fPGxxwZf3N3N6VT7uVAe767rsWVyXn1QGDEzrHuqoWTjvD4YHzfyq7+yjIj/DbuhYPaLIwvrhll4qI94jqyrzUEpxzvwie0mcFbOinxBr+dkL+/EHw7x1qI333/maPSwH8MmzjWX9I6c4723oJjVF2ZM0YklxKEqy3fbyKEZdNZ1FZdl0+4LUd/rYXd9FMKy5+b7Ndj1rZ20nT26r49tP7CLb7eTxLbWD1g1LJkkiMdR39nHrg1vZ39jDT1/YT3VTDzXtfYM+nD++bjkPffYMFpV68AXCPL+7gZUVuXznqiW8b2U5X7l0IQB55tCJUrBmzuizciyXLyvlmS+eQ3luul1kP60yj0BI8/bRNspzB2Z2RY5952em0dsfYs2cfK5cXkZlQSbP7WqgwxtgZUWu/QGz3vQrK/Lsln5BZhrluemsrMjlQFMP2W4n84o9g+IqNh9fnO1myYxsdn7/UpaZZ6EP3MfN9u9dwr7/vJyfr10Z9/9snc+xuMxjD2lZ48S3XryAG8+ew4fN+sdQVtKZU5TJaZX5eNxOvnvV0kEJ7ORZuTz+uTO5fFlZzBis5eCtg0BhljGcBXDBwiIAnCkO/vX02fzvv67ivk+dTkaa016SJCc9lXPmF9l/7/SqAv584xqyXKksnZFNxZAaUyzWcJb1/1usZDUzL8O+rTArzX5dTq8qsKf1xpLmTMEfMHoi84qzmFOYSYc3QFtvP+v2NuFQA+c13GgeFH//yiH2NnTz1csW8qWL59MfCtsF9/94ajfP7qynsdtHXkbqsDPxr1heRn8wzEsR4/oNnT72NnRzwcJiFpVms6uu057QcbnZG442xAbGhA9r6rU1Vdf6m2W56fbnwUoUnsjhrJZe5hRk2AnGmrUW+dleUOLB5XTYSSQcMQ14b0MXj26u4eNnVfLr61dypNXLszsb+Mtbx1hSls0Fi4pIcSh7eA6M4cC5RVn29OqRlOa4qe/0obVReyrJdtux7qjtpLqph9PM88s+fmYlKQ7FjtpOHtlUQ2VBBo997kwC4TD3vnFk1OcaD6MPzCaBUurHwFVAP3AQ+ITWusO87RvAjUAI+Det9XPm9lOBe4B04O/AF/Voi1clKBgK88X7t+IPhrlmxQye2FrH5qPteFxOLl4y0Iq0DhoLzCJ7uzfA8pk5fGR1BR9ZPTADq8AcUlpSlm3XLOLhcCj7OhNnzy/kW1cs5oOnzmT1HS/RHwozI2Jm1+xBScSFUvCdq5aglGJBSRbPmeennDI7z05I6/c1oZTRWlszJ5+nttXZB7jHbj6ToFmwSx3Si1g2I4fMtBQWmv93rKU7RuplxfKvZ8xmbnEWxR43C0o8/H1Hg32gdDgU375ySczH2j2RgkwqCzPZ/t1Losaw0ly9NxZrJo91wmV+ZhrzirPITEvhvAVFMR9nTcl970mlww4WZ8wtYP1Xz497qizAxYtLCITCwxKC9b4rzXHT1x+ipr2PwiwXTofxnBfGMVzmcjqoaffS5QsytyjLPugeau5h/f5mzpxbSLHHxWsHW7h8WSmZaSn8bXs9eRmpXL6sDHdqCqtm5/HS3kbWrp7F3a8dprq5B5fTEbWXd2pFHiXZLp7d2cA1K4ze4Bbz5NDTqwo42ublqW11lOelk+UyzqUozHLZU7SHOtray7LyHI609LK9pgOYDRgt90Wl2Tgc8NbhNjuJpDgUHpeTg83GStSVhZmcNa+QRaUee3+dMbeAFIciPdU4t6Y0x23XRP7r2b28uKeRF249j9+uP0iWy8nnzp9LtjuVyoIM/vPpPTR3+/nRB5bjcqZQkZ/BweYeDrf0UpCVxp76LnsoezRlOW521nbay9iX5bhZYDbk/r6jnmBY87EzKvnJdSuYmZfOm4da2Xikna3HO7h+TQXzij1cuqSUP795jFsumGe/X5JlUpII8ALwDa11UCn1Q+AbwNeVUkuAtcBSYAbwolJqgdY6BPwGuAl4EyOJXAY8k4zgHEpx4eJirl9TwQULi/nH3iZae/386cY1duErkjVTC7CLiJGsoZB4zg2IxeNO5VPmdSdOmZ3Lm4eMnoj14a+MaN1+8uxKLllSwtIZRu9gYYmH53Y14nE5mVeURUhrlDI+cFWFmWS6nJxeZcQ220xKSim7pTfUSTNz2PUflyX8v4xkZl4GH1plxPDRNbMpz00f9aQ8y9nzCrlieRlnmPs5kSQGxsyubLfTLiYXZLm4dkU5Fy8piTq332K9zteaB8mhYi2zEssZcwvs/yWSNZxVlu0mYNYDijwuKvKNcxAuNYcjR5LmdLD1eAcOZTyPlajuXH+Q6qYe1p42i4+fWUlvfwhnioOFpR42H+vg/afMtOtR5y0o4r+f389DbxurLeyo6aAiPyPq6+VwKM6eV8T6fU1orVFKsauuixSHMd116Yxs/vLWMR7bXMv1aypQSg0aYhvqaKuXNXPyyXan2qsnBEJhmnv8lOa47YS+OOJiX9npqfZqAZWFmczKz+DZL51r3+5xp3JqRZ59ommJx01jl9EjeHJrHQ1dPp7aVsczOxv40KpZ9uSMfzl9Nv/59B5yM1LtBTXnFmXyxsFWLv3ZP5mZn05jl3/UeohlRm46L+xutIe0SnPc5GSkGmu9mY3BxWUeu8F3UnkOD5uX4D7XbOR89vy5zC/JshuCyTQpw1la6+e11tbg6JuANWh9DfCA1tqvtT4MVAOrlVJlQLbW+g2z9/FH4NpkxedwKD573lyuXVlOTkYq935yNY989ky72ztUlsvJTLP4eXKUJJKbkcbPPnwyN507d1ziO9ucHlqea9QK7njfMi5aPNBDOnNuIR86bWDIZ4HZGltRkYvDoUhNcdgHPOuNPbcok3PmF3L+wviKvhOhyOOyp+7GoyDLxf9cf8qIM6/i4XAonr/1PL522UJuPn8u2W4nDocaMYGA0Uh47kvnsqYq8cZCPKzCekmO2x7OLMxysaoyny3ffs+w2VzRWMXnj51RyaLSbMpz0/n8BfPsIc7zFxbjTHHYtYQlM4z3yUdWD7we1nvlty8b02zbvQH2NHTbS7QMtaoyj9be/oiieCfzirJwp6awzGzwnLegiO+bU18Xl3rYUdtpzwJ7/WALn/i/Dazb10RdZx+zCzJZWZHL/sZudtZ20tTttyeZXHXyDD53/lwWlw008CoLM+j2BTijqoBTZkXvjf7wg8v56YdPBqAs183B5h62Hu+wz7+5/fEd+INhrouos1136iw8bicfXVNhJ9i5xVm0ewMUZ7vsc68WlXmIR2m2G38wbJ97ZvXEF5Z66AuESHM6Bg1fW6tipzkdnG4Ol6+Ylcu/X7Jw1PfseJisnkikTwIPmj+XYyQVS425LWD+PHR7VEqpmzB6LVRUVMS6W9xOGWX4A4wieDCkKc2J3mp+38qZUbcn4vKTynhkUw3LZ+WS4lB8dM3sEe9vDTutnDWQ4EqyXTR3++2Dg1KKP924ZtxinO5Kc9x87vx5Y3qMdRJZslk1kbIcN1WFmTy93WMPa8bb+yrITKMsx82/X7LA3vbl9yzgaJuX6iaj2B7p0+dUcUpF3qD62JKybAqz0mjp6WdlRS5bjnXQHwzHnLSwyly9duPRdqqKsthV12U3iJbPzOF3H1vF2fMK7eHTm86by8Obavjm4zvIzUizp+Nur+lEayMpnDO/iPs3HOPfHthiJ5/SHDdlOel87bJFg57/nk+sJhTW9oE+msjp7O9bWc4TW+v42iPbUQrWnjaL+zccZ2GJh5MiaoA5Gam88rULBvU0L1xYzJZjHfz8wytYt6+JX7x4gOXlwxuY0VgzHq2VG6z9ubDUw/p9zSwoyRpU57PqkWvm5Nu91ImUtCSilHoRiNavvl1r/YR5n9uBIHCf9bAo99cjbI9Ka30XcBfAqlWrJuRMnG9fuXjCLr85tyiL9V+9YEz3/+qlC3n/KQN5t8TjZieDlyMR00N6RBKZV+zhuVvPHeURw/3HtcsIBAfXWxwOxS/XriCshyej2QWZw6Z6OxyKcxcU8djmWm45fx4337eJQEgPWvct0tyiLHIzUtl0pJ0LFhbTNKQR854lg2etleem8++XLOT//W03Tofi1osXUORx8c3Hd9gx5Wem8bMPreCjf3iLW+7bDAw/r8ZirDsW/z46b0ERy8qz2VnbxYpZudxywTwejRhuizS097umqoCHPnMGYAzLXr96+GNisS6S9jdzSrSVRKzi+qLSwZ/ZJWXZFGSmceXy2JNFkilpSURrffFItyulbgCuBC6KKJDXAJHjFzOBOnP7zCjbp4yhH7CpxOFQ3HLB4FZ1idljsj7EYvqYV5RFscdln+iZiFjDHEopYpTCovromgq6+oKcs6CQBSUedtV1xaxhORyKUyvyePtom33exGjvvxvOmI0vEOKc+YUsn5lLMBTmNy9Xc7ytz64DnjmvkN/+y6n8/MUD0O4d8fyYsVBKmclxMxctKmZmXgav33ahPWV/rH8rXgtLPXz8zEruef0IhVkue5LGwhJjXy0a0tt1p6aw4faLcSRWAnzHJmt21mXA14HztNaRZxM9CfxFKfVTjML6fGCD1jqklOpWSp0OvAV8DPjVRMf9bnLx4mK6fUGKY4xfi6nrkqWl9rk8k+3U2fn8/gajVrh8Zo6RREZ4T51amcdLe5t4Ybdx9vXSspyY9wVjKnVkA8iZ4uD7Vy/lb9vrB7X+L11ayiVLSgiEdFzTaON16dJS/vu6k7lkqdFLKoyxAsB4u/2Kxexv7B50AvLiMg93vG8ZVy4ffjXMoScqTySVpFmyIz+pUtWAC7DWQXhTa/1Z87bbMeokQeBLWutnzO2rGJji+wzwhXim+K5atUpv3Lhx3P8HIcRgf91Sy1cf2cYb37go5sF2T30XV/3qVYJhzcy8dF79+oUTHOX0YV2tM94TdcebUmqT1nrVqPebjCQykSSJCDExtNY0dvljTi6xHGzu4b43j7GgJIu1q9/5xBeRHPEmkakwO0sI8S6glBo1gYBRYP/OVbFPGhXTiyx7IoQQImHv+uEspVQzEN8VdYYrBFrGMZzxInGNjcQ1NhLX2Lxb45qttY69zo/pXZ9E3gml1MZ4xgQnmsQ1NhLX2EhcY3OixyXDWUIIIRImSUQIIUTCJImM7K7JDiAGiWtsJK6xkbjG5oSOS2oiQgghEiY9ESGEEInTWsvXkC+MC17tw7ieyW1Jeo5ZwDpgD7AL40qNAN8DaoGt5td7Ix7zDTOmfcClEdtPBXaYt/2SgR6mC2OZ/WqMNccq44ztiPn3tgIbzW35GBcTO2B+z5vIuICFEftkK9AFfGky9hdwN9AE7IzYNiH7B7jBfI4DwA1xxPVjYC+wHXgcyDW3VwJ9EfvttxMc14S8bgnE9WBETEeArZOwv2IdGyb9PRb185CMA+R0/gJSMC7ZWwWkAduAJUl4njLgFPNnD7AfWGJ+uL4S5f5LzFhcwBwzxhTztg3AGRhL5j8DXG5u/5z1Zse4YuSDccZ2BCgcsu1HmAkVuA344UTHNeQ1asC4JuqE7y/gXOAUBh98kr5/MA4ih8zveebPeaPEdQngNH/+YURclZH3G/L/TURcSX/dEolrSCw/Ab4zCfsr1rFh0t9j0b5kOGu41UC11vqQ1rofeADjiovjSmtdr7XebP7cjdHqiHmhLRK76uM1wL3mz48AF6lErxk7+G/dO+Q5Jjqui4CDWuuRTiJNWlxa638CbVGeL9n751LgBa11m9a6HaM1al+nOFpcOvZVRKOaqLhGMKn7K2I/KOBDwP0jBZukuGIdGyb9PRaNJJHhyoHjEb+PeBXF8aCUqgRWYnQrAT6vlNqulLpbKWVdVjFWXOXEvuqj/RjzQNIJxHPtVg08r5TaZF4lEqBEa11v/q16wLqO7kTGZVnL4A/3ZO8vmJj9807fm5/EaI1a5iiltiilXlZKnRPx3BMVV7Jft3eyv84BGrXWByK2Tfj+GnJsmJLvMUkiw43pKorv+MmUygIexVj2vgv4DTAXWAHUY3SpR4prpHgT/V/O0lqfAlwO3KKUGunSeRMZF0qpNOBq4GFz01TYXyMZzzjeyX4behXReqBCa70S+DLGdXyyJzCuiXjd3snr+REGN1QmfH9FOTbEMqn7TJLIcLGurjjulFKpGG+S+7TWjwForRu11iGtdRj4Hcbw2khxjXTVR/sxSiknkEMcwwpa6zrzexNGMXY10Gh2j60ufNNEx2W6HNistW40Y5z0/WWaiP2T0Hsz4iqiHzWHNTCHPlrNnzdhjKMvmKi4Juh1S3R/OYH3YxSerXgndH9FOzYwVd9jIxVMTsQvjOXxD2EUqKzC+tIkPI/CGKP8+ZDtZRE/34ox1gmwlMHFs0MMFM/eBk5noHj2XnP7LQwunj0UR1yZgCfi59cxxkR/zOCi3o8mMq6I+B4APjHZ+4shhdaJ2D8Yxc7DGAXPPPPn/FHiugzYDRQNuV9RRBxVGDOl8icwrqS/bonEFbHPXp6s/UXsY8OUeI8N+yy804Phu/ELeC/GjIiDwO1Jeo6zMbqJ24mY5gj8CWNK3naMywVHfthuN2PahznLwty+Cthp3vZrBqbxuTGGfaoxZmlUxRFXlfmG3IYxvfB2c3sB8BLGtL+Xhrzpkx6X+bgMjKth5kRsm/D9hTHMUQ8EMFpuN07U/sGoa1SbX5+II65qjDHuQVNTgQ+Yr+82YDNw1QTHNSGv21jjMrffA3x2yH0ncn/FOjZM+nss2pecsS6EECJhUhMRQgiRMEkiQgghEiZJRAghRMIkiQghhEiYJBEhhBAJkyQixDhTSt2ulNplLumxVSm1Rin1JaVUxmTHJsR4kym+QowjpdQZwE+B87XWfqVUIcZJq68Dq7TWLSOLXLoAAAF6SURBVJMaoBDjTHoiQoyvMqBFa+0HMJPGB4EZwDql1DoApdQlSqk3lFKblVIPm+skoZQ6opT6oVJqg/k1b7L+ESHiIUlEiPH1PDBLKbVfKXWnUuo8rfUvMdYfukBrfYHZO/kWcLE2FrrciLGon6VLa70a4wzjn0/0PyDEWDgnOwAh3k201j1KqVMxlhK/AHhQKXXbkLudjnEhodfMy5WkAW9E3H5/xPefJTdiId4ZSSJCjDOtdQhYD6xXSu3AuNxoJIVx4Z+PxPoTMX4WYsqR4SwhxpFSaqFSan7EphXAUaAb41KnYFxh8Cyr3qGUylBKLYh4zIcjvkf2UISYcqQnIsT4ygJ+pZTKxbgIVDVwE8ZFjp5RStWbdZGPA/crpVzm476FsXI0gEsp9RZGIy9Wb0WIKUGm+AoxhSiljiBTgcU0IsNZQgghEiY9ESGEEAmTnogQQoiESRIRQgiRMEkiQgghEiZJRAghRMIkiQghhEiYJBEhhBAJ+/8BPPdNrPH2dA4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import gym_bubbleshooter\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.metrics import py_metrics\n",
    "from absl import flags\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "print(\"load env ..\")\n",
    "\n",
    "env_name =(\"BubbleShooter-v0\")\n",
    "\n",
    "print(\"load env completed\")\n",
    "\n",
    "num_iterations = 200000  # @param\n",
    "\n",
    "initial_collect_steps = 1000  # @param\n",
    "collect_steps_per_iteration = 1  # @param\n",
    "replay_buffer_capacity = 100000  # @param\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "batch_size = 64  # @param\n",
    "learning_rate = 1e-3  # @param\n",
    "log_interval = 200  # @param\n",
    "\n",
    "num_eval_episodes = 10  # @param\n",
    "eval_interval = 1000  # @param\n",
    "\n",
    "\n",
    "\n",
    "env = suite_gym.load(env_name, discount=0.99, max_episode_steps=1000)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "#print('Observation Spec:')\n",
    "#print(env.time_step_spec().observation)\n",
    "#print('Action Spec:')\n",
    "#print(env.action_spec())\n",
    "\n",
    "time_step = env.reset()\n",
    "#print('Time step:')\n",
    "#print(time_step)\n",
    "\n",
    "action = 1\n",
    "\n",
    "next_time_step = env.step(action)\n",
    "#print('Next time step:')\n",
    "#print(next_time_step)\n",
    "train_py_env = suite_gym.load(env_name, discount=0.99, max_episode_steps=1000)\n",
    "eval_py_env = suite_gym.load(env_name, discount=0.99, max_episode_steps=1000)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "\n",
    "\n",
    "train_env.reset()\n",
    "#print('Observation Spec:')\n",
    "#print(train_env.time_step_spec().observation)\n",
    "#print('board:')\n",
    "#print(\"env\")\n",
    "#print(eval_env.reset())\n",
    "#print('end')\n",
    "\n",
    "\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "        train_env.observation_spec(),\n",
    "        train_env.action_spec(),\n",
    "        fc_layer_params=fc_layer_params)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = dqn_agent.DqnAgent(\n",
    "        train_env.time_step_spec(),\n",
    "        train_env.action_spec(),\n",
    "        q_network=q_net,\n",
    "        optimizer=optimizer,\n",
    "        td_errors_loss_fn=dqn_agent.element_wise_squared_loss,\n",
    "        train_step_counter=train_step_counter)\n",
    "\n",
    "tf_agent.initialize()\n",
    "\n",
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())\n",
    "print(\"policy\")\n",
    "\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "print(\"compute_avg_return ... \")\n",
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)\n",
    "#compute_avg_return(eval_env, random_policy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        data_spec=tf_agent.collect_data_spec, \n",
    "        batch_size=train_env.batch_size,\n",
    "        max_length=replay_buffer_capacity)\n",
    "\n",
    "print(\" init buffer ... \")\n",
    "def collect_step(environment, policy):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, random_policy)\n",
    "print(\"collected\")\n",
    "dataset = replay_buffer.as_dataset(\n",
    "        num_parallel_calls=3, sample_batch_size=batch_size,\n",
    "        num_steps=2).prefetch(3)\n",
    "\n",
    "print(\"dataset\")\n",
    "iterator = iter(dataset)\n",
    "\n",
    "print(\"train\")\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "\n",
    "print(\"save\")\n",
    "\n",
    "#flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'), 'Root directory for writing logs/summaries/checkpoints.')\n",
    "#flags.DEFINE_integer('num_iterations', 100000, 'Total number train/eval iterations to perform.')\n",
    "#flags.DEFINE_bool('use_ddqn', False, 'If True uses the DdqnAgent instead of the DqnAgent.')\n",
    "#flags.mark_flag_as_required('root_dir')\n",
    "\n",
    "train_checkpoint_interval=10000\n",
    "policy_checkpoint_interval=5000\n",
    "rb_checkpoint_interval=2000\n",
    "log_interval=1000\n",
    "summary_interval=1000\n",
    "summaries_flush_secs=10\n",
    "agent_class=dqn_agent.DqnAgent\n",
    "debug_summaries=False\n",
    "summarize_grads_and_vars=False\n",
    "\n",
    "#FLAGS = flags.FLAGS\n",
    "#root_dir = FLAGS.root_dir\n",
    "root_dir = \"/home/leiningc/project/data\"\n",
    "root_dir = os.path.expanduser(root_dir)\n",
    "train_dir = os.path.join(root_dir, 'trainbubble')\n",
    "eval_dir = os.path.join(root_dir, 'eval')\n",
    "print(\"root_dir\", root_dir)\n",
    "print(\"train_dir\", train_dir)\n",
    "\n",
    "\n",
    "train_metrics = [\n",
    "        tf_metrics.NumberOfEpisodes(),\n",
    "        tf_metrics.EnvironmentSteps(),\n",
    "        tf_metrics.AverageReturnMetric(),\n",
    "        tf_metrics.AverageEpisodeLengthMetric(),\n",
    "        ]\n",
    "\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "#train_summary_writer = tf.compat.v2.summary.create_file_writer(train_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "#train_summary_writer.set_as_default()\n",
    "train_checkpointer = common.Checkpointer(\n",
    "        ckpt_dir=train_dir,\n",
    "        agent=tf_agent,\n",
    "        global_step=global_step,\n",
    "        metrics=metric_utils.MetricsGroup(train_metrics,\n",
    "            'train_metrics'))\n",
    "\n",
    "\n",
    "\n",
    "print(\"End save para\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "   \n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, tf_agent.collect_policy)\n",
    "        \n",
    "    # Sample a batch of data from the buffer and update the\n",
    "    # agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = tf_agent.train(experience)\n",
    "    step = tf_agent.train_step_counter.numpy()\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        print(\"save the agent\")\n",
    "        train_checkpointer.save(global_step=step)\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n",
    "\n",
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/home/leiningc/anaconda3/envs/sim/lib/python35.zip',\n",
       " '/home/leiningc/anaconda3/envs/sim/lib/python3.5',\n",
       " '/home/leiningc/anaconda3/envs/sim/lib/python3.5/plat-linux',\n",
       " '/home/leiningc/anaconda3/envs/sim/lib/python3.5/lib-dynload',\n",
       " '/home/leiningc/anaconda3/envs/sim/lib/python3.5/site-packages',\n",
       " '/home/leiningc/anaconda3/envs/sim/lib/python3.5/site-packages/IPython/extensions',\n",
       " '/home/leiningc/.ipython',\n",
       " '/home/leiningc/project/gym_env/gym-bubbleshooter']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load env ..\n",
      "load env completed\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import gym_bubbleshooter\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "print(\"load env ..\")\n",
    "env_cart = 'CartPole-v0'  # @param\n",
    "env_name =(\"BubbleShooter-v0\")\n",
    "num_iterations = 8000  # @param\n",
    "print(\"load env completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_collect_steps = 1000  # @param\n",
    "collect_steps_per_iteration = 1  # @param\n",
    "replay_buffer_capacity = 100000  # @param\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "batch_size = 64  # @param\n",
    "learning_rate = 1e-3  # @param\n",
    "log_interval = 200  # @param\n",
    "\n",
    "num_eval_episodes = 10  # @param\n",
    "eval_interval = 1000  # @param\n",
    "\n",
    "\n",
    "env = suite_gym.load(env_name, discount=0.99, max_episode_steps=1000)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "\n",
    "\n",
    "train_py_env = suite_gym.load(env_name, discount=0.99, max_episode_steps=1000)\n",
    "eval_py_env = suite_gym.load(env_name, discount=0.99, max_episode_steps=1000)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "\n",
    "\n",
    "train_env.reset()\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    " \n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"BubbleShooter-v0\")\n",
    "state = env.reset()\n",
    "len(state['board'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=dqn_agent.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_agent.train = common.function(tf_agent.train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-af1a85f86762>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sim/lib/python3.5/site-packages/tf_agents/environments/py_environment.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m           \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \"\"\"\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sim/lib/python3.5/site-packages/tf_agents/environments/wrappers.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sim/lib/python3.5/site-packages/tf_agents/environments/py_environment.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m           \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \"\"\"\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sim/lib/python3.5/site-packages/tf_agents/environments/gym_wrapper.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_obs_space_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m       \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_obs_space_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sim/lib/python3.5/site-packages/tf_agents/environments/gym_wrapper.py\u001b[0m in \u001b[0;36m_to_obs_space_dtype\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mmatched_observations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_obs_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m       \u001b[0mmatched_observations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     return tf.nest.pack_sequence_as(self._observation_spec,\n\u001b[1;32m    191\u001b[0m                                     matched_observations)\n",
      "\u001b[0;32m~/anaconda3/envs/sim/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'dict'"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArraySpec(shape=(196,), dtype=dtype('int32'), name=None, minimum=[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       " 0 0 0 0 0 0 0 0 0 0 0], maximum=[6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
       " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
       " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
       " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
       " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
       " 6 6 6 6 6 6 6 6 6 6 6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.observation_spec()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_env = suite_gym.load(env_cart, discount=0.99, max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "OrderedDict([('board', BoundedArraySpec(shape=(196,), dtype=dtype('int32'), name=None, minimum=[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0], maximum=[6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]))])\n",
      "Action Spec:\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "print('Action Spec:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "OrderedDict([('board', BoundedArraySpec(shape=(196,), dtype=dtype('int32'), name=None, minimum=[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0], maximum=[6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6])), ('next_bubble', BoundedArraySpec(shape=(), dtype=dtype('int64'), name=None, minimum=0, maximum=6))])\n",
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name=None, minimum=0, maximum=178)\n",
      "Time step:\n",
      "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=OrderedDict([('board', array([0, 6, 2, 0, 2, 2, 4, 2, 1, 4, 4, 3, 2, 1, 2, 6, 5, 1, 5, 1, 4, 4,\n",
      "       6, 0, 4, 5, 4, 3, 0, 6, 4, 6, 0, 0, 1, 3, 6, 5, 5, 4, 1, 2, 5, 2,\n",
      "       3, 3, 2, 1, 0, 4, 6, 5, 6, 4, 0, 5, 6, 1, 2, 4, 4, 6, 4, 5, 1, 2,\n",
      "       4, 3, 1, 4, 5, 4, 5, 5, 0, 3, 6, 0, 4, 5, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7], dtype=int32)), ('next_bubble', array(5))]))\n",
      "Next time step:\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(40., dtype=float32), discount=array(0.99, dtype=float32), observation=OrderedDict([('board', array([0, 6, 2, 0, 2, 2, 4, 2, 1, 4, 4, 3, 2, 1, 2, 6, 5, 1, 5, 1, 4, 4,\n",
      "       6, 0, 4, 5, 4, 3, 0, 6, 4, 6, 0, 0, 1, 3, 6, 5, 5, 4, 1, 2, 5, 2,\n",
      "       3, 3, 2, 1, 0, 4, 6, 5, 6, 4, 0, 7, 6, 1, 2, 4, 4, 6, 4, 5, 1, 2,\n",
      "       4, 3, 1, 4, 5, 4, 7, 7, 0, 3, 6, 0, 4, 5, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7], dtype=int32)), ('next_bubble', array(5))]))\n",
      "Observation Spec:\n",
      "OrderedDict([('board', BoundedTensorSpec(shape=(196,), dtype=tf.int32, name=None, minimum=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "      dtype=int32), maximum=array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
      "      dtype=int32))), ('next_bubble', BoundedTensorSpec(shape=(), dtype=tf.int64, name=None, minimum=array(0), maximum=array(6)))])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "env.reset()\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "print('Action Spec:')\n",
    "print(env.action_spec())\n",
    "\n",
    "time_step = env.reset()\n",
    "print('Time step:')\n",
    "print(time_step)\n",
    "\n",
    "action = 1\n",
    "\n",
    "next_time_step = env.step(action)\n",
    "print('Next time step:')\n",
    "print(next_time_step)\n",
    "\n",
    "train_py_env = suite_gym.load(env_name, discount=0.99, max_episode_steps=1000)\n",
    "eval_py_env = suite_gym.load(env_name, discount=0.99, max_episode_steps=1000)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "\n",
    "\n",
    "train_env.reset()\n",
    "print('Observation Spec:')\n",
    "print(train_env.time_step_spec().observation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_train_py_env = suite_gym.load(env_cart, discount=0.99, max_episode_steps=1000)\n",
    "cart_eval_py_env = suite_gym.load(env_cart, discount=0.99, max_episode_steps=1000)\n",
    "\n",
    "cart_train_env = tf_py_environment.TFPyEnvironment(cart_train_py_env)\n",
    "cart_eval_env = tf_py_environment.TFPyEnvironment(cart_eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name, discount=0.99, max_episode_steps=1000)\n",
    "eval_py_env = suite_gym.load(env_name, discount=0.99, max_episode_steps=1000)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.tensor_spec.BoundedTensorSpec"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob = train_env.observation_spec()\n",
    "type(ob['board'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.tensor_spec.BoundedTensorSpec"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cart_train_env.observation_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(4,), dtype=tf.float32, name=None, minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
       "      dtype=float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart_train_env.observation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n board\n"
     ]
    }
   ],
   "source": [
    "for x in ob:\n",
    "    print(\"n\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(196,), dtype=tf.int32, name=None, minimum=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int32), maximum=array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       "      dtype=int32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.observation_spec()[\"board\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(4,), dtype=tf.float32, name=None, minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
       "      dtype=float32)))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart_train_env.time_step_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=<tf.Tensor: id=18, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: id=19, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: id=20, shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=OrderedDict([('board', <tf.Tensor: id=21, shape=(1, 224), dtype=int32, numpy=\n",
       "array([[5, 3, 3, 6, 6, 5, 4, 2, 1, 1, 3, 3, 1, 1, 0, 6, 1, 6, 1, 4, 3, 3,\n",
       "        1, 0, 0, 6, 5, 1, 1, 6, 1, 6, 5, 1, 2, 5, 6, 5, 6, 3, 0, 1, 5, 5,\n",
       "        6, 6, 1, 6, 4, 3, 0, 4, 0, 6, 2, 6, 5, 4, 4, 5, 0, 5, 4, 6, 6, 6,\n",
       "        5, 0, 2, 0, 3, 4, 3, 6, 4, 6, 1, 5, 5, 3, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7]], dtype=int32)>)]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=OrderedDict([('board', BoundedTensorSpec(shape=(196,), dtype=tf.int32, name=None, minimum=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int32), maximum=array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       "      dtype=int32)))]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.time_step_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y TensorSpec(shape=(), dtype=tf.int32, name='step_type')\n",
      "y TensorSpec(shape=(), dtype=tf.float32, name='reward')\n",
      "y BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))\n",
      "y OrderedDict([('board', BoundedTensorSpec(shape=(196,), dtype=tf.int32, name=None, minimum=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "      dtype=int32), maximum=array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
      "      dtype=int32))), ('next_bubble', BoundedTensorSpec(shape=(), dtype=tf.int64, name=None, minimum=array(0), maximum=array(6)))])\n"
     ]
    }
   ],
   "source": [
    "for x in train_env.time_step_spec():\n",
    "    print(\"y\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('board',\n",
       "              BoundedTensorSpec(shape=(196,), dtype=tf.int32, name=None, minimum=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                    dtype=int32), maximum=array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                     6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                     6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                     6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                     6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                     6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                     6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                     6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                     6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       "                    dtype=int32)))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.time_step_spec().observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=<tf.Tensor: id=8, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: id=9, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: id=10, shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=OrderedDict([('board', <tf.Tensor: id=11, shape=(1, 224), dtype=int32, numpy=\n",
       "array([[1, 4, 5, 3, 0, 5, 3, 5, 3, 2, 1, 6, 3, 2, 2, 3, 3, 4, 4, 5, 2, 0,\n",
       "        4, 2, 4, 6, 2, 4, 6, 5, 4, 3, 1, 3, 3, 1, 5, 2, 0, 0, 3, 1, 5, 3,\n",
       "        2, 0, 5, 2, 2, 0, 6, 6, 4, 5, 2, 5, 5, 2, 5, 4, 0, 5, 1, 0, 0, 4,\n",
       "        2, 6, 5, 3, 1, 4, 5, 6, 2, 6, 0, 3, 3, 2, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7]], dtype=int32)>)]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = train_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('board',\n",
       "              BoundedTensorSpec(shape=(196,), dtype=tf.int32, name=None, minimum=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                    dtype=int32), maximum=array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                     6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                     6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                     6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                     6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                     6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                     6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                     6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                     6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       "                    dtype=int32)))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.observation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=<tf.Tensor: id=12, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: id=13, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: id=14, shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: id=15, shape=(1, 4), dtype=float32, numpy=\n",
       "array([[-0.00490616,  0.03789723, -0.02988659, -0.03203917]],\n",
       "      dtype=float32)>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart_train_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=<tf.Tensor: id=8, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: id=9, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: id=10, shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=OrderedDict([('board', <tf.Tensor: id=11, shape=(1, 224), dtype=int32, numpy=\n",
       "array([[1, 4, 5, 3, 0, 5, 3, 5, 3, 2, 1, 6, 3, 2, 2, 3, 3, 4, 4, 5, 2, 0,\n",
       "        4, 2, 4, 6, 2, 4, 6, 5, 4, 3, 1, 3, 3, 1, 5, 2, 0, 0, 3, 1, 5, 3,\n",
       "        2, 0, 5, 2, 2, 0, 6, 6, 4, 5, 2, 5, 5, 2, 5, 4, 0, 5, 1, 0, 0, 4,\n",
       "        2, 6, 5, 3, 1, 4, 5, 6, 2, 6, 0, 3, 3, 2, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7]], dtype=int32)>)]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0716 14:16:13.832557 140373639632640 module_wrapper.py:126] From /home/leiningc/anaconda3/envs/sim/lib/python3.5/site-packages/tensorflow_core/python/util/module_wrapper.py:153: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Received a mix of batched and unbatched Tensors, or Tensors are not compatible with Specs.  num_outer_dims: 1.\nSaw tensor_shapes:\n   TimeStep(step_type=TensorShape([1]), reward=TensorShape([1]), discount=TensorShape([1]), observation=OrderedDict([('board', TensorShape([1, 224]))]))\nAnd spec_shapes:\n   TimeStep(step_type=TensorShape([]), reward=TensorShape([]), discount=TensorShape([]), observation=OrderedDict([('board', TensorShape([196]))]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-62368a0d2b62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrandom_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/sim/lib/python3.5/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sim/lib/python3.5/site-packages/tf_agents/policies/random_tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mouter_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_outer_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_time_step_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     action_ = tensor_spec.sample_spec_nest(\n",
      "\u001b[0;32m~/anaconda3/envs/sim/lib/python3.5/site-packages/tf_agents/utils/nest_utils.py\u001b[0m in \u001b[0;36mget_outer_shape\u001b[0;34m(nested_tensor, spec)\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0mnum_outer_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   if not is_batched_nested_tensors(\n\u001b[0;32m--> 341\u001b[0;31m       nested_tensor, spec, num_outer_dims=num_outer_dims):\n\u001b[0m\u001b[1;32m    342\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sim/lib/python3.5/site-packages/tf_agents/utils/nest_utils.py\u001b[0m in \u001b[0;36mis_batched_nested_tensors\u001b[0;34m(tensors, specs, num_outer_dims)\u001b[0m\n\u001b[1;32m    129\u001b[0m       \u001b[0;34m'And spec_shapes:\\n   %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m       (num_outer_dims, tf.nest.pack_sequence_as(tensors, tensor_shapes),\n\u001b[0;32m--> 131\u001b[0;31m        tf.nest.pack_sequence_as(specs, spec_shapes)))\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Received a mix of batched and unbatched Tensors, or Tensors are not compatible with Specs.  num_outer_dims: 1.\nSaw tensor_shapes:\n   TimeStep(step_type=TensorShape([1]), reward=TensorShape([1]), discount=TensorShape([1]), observation=OrderedDict([('board', TensorShape([1, 224]))]))\nAnd spec_shapes:\n   TimeStep(step_type=TensorShape([]), reward=TensorShape([]), discount=TensorShape([]), observation=OrderedDict([('board', TensorShape([196]))]))"
     ]
    }
   ],
   "source": [
    "random_policy.action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_bubbleshooter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BubbleShooter-v0\")\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state['board'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict(board:MultiDiscrete([7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
       " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
       " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
       " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
       " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
       " 7 7 7 7 7 7 7 7 7 7 7]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.observation_space\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gym.spaces.dict_space.Dict"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = spaces.MultiDiscrete([x for x in range(1,100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}



Hello My name is Christian Leininger and I am going to present you the work from  Girdhar el al.
its called the Video Action Transformer Network.

The objective of this work is to localize all humans and their associated action in a given  video clip.

To achive this the author proposed  a model  called " Action Transformer " which is
the main contribution of this work.

It is able to extract contextual Informatation without supervision.
(there are no labels during the training phase)

This new model uses  an new method called attention to over come the limitations
of the so far used RNNs



Image:

One reason why inferring the actions of persons in video clips  is 
challaneging. is that the model needs to  able to
capture the information over time (temporal) and contextual information (other
people, other objects). 
Example action are "watching a person ", "listen to a person", "pointing to an
object" and many more. 
So far RNN have been used for this task. Professor Ronneberger already explained
in the lecture before Chrismas that one issue of this method is that the
information needs to be processd sequenicely which increase the compute time.
The serious issue is that in the RNN method the information collected over time
is saved in one vector. This leads to to information lost when the input
sequence is too long. Like it your case. To overcame this issue the authors
propose this Video Action Transformer architecure that uses a techique called attention.
This is inspiered by the The Transformer architecture from Vaswani et al. which
has been successful apllied to  sequence modelling tasks like language
translation. The attention method I will explain later in more detail, but one
drawback is that the compute cost for the basic attention method is O(n2) where
n is the amout of pixel of the inputImage. A major contribution of this work is
that the Video Action Transformer does not requiere so much computational time



3.Slide Image 


This Image is from the original paper and should illustrate what the model is
learning. 

On the right Image is demonstarted how the output of the model looks like.
We can see that the model creates an bounding box for each person it is
reconizing and what is the action with the highest probabilty azosatied to this
person.


In the bottom left image it is illustradet for the hilighted bounding box there 
the network is attenting too. In this case it the heands and faces of other persons in the image but also on
it self.







4. Model overview


This Image shows an overview of the proposed Video Action Transformer Network.
First I will give brief overview and then I will explain every part in detail
We can see the data flows from left to right. On the left we can see the model
takes a video clip as input passed through the trunk. Where a pretrained CNN 
generate a feature map. A Region Proposal Network uses the center frame to
generate  multiple potential person bounding boxes as an Input for the head .
The intuition behind the Action Transformer is that will use the context an
vector with a probably for each action class and and make a tighter bounding box
for each person. Here N are the number of different action classes and plus 1 
respresents the backround

______________________________________________________________________________________________
7. Trunk Region 


The video input clip is Transformt in a so called T Frame that encodes around 3
seconds of the keyframe. The shape of the Input is TxHxW where H and W are
height and Wight of an Image and T is the amount of Images stacked which is
typcilaly 64. This is a hyperparameter needs to be selected. Later in the reults
we will see how this will change the performence of the model.

8. I3D base  

The Trunk of the model is a CNN pretrained on the kinetics-400 dataset. Which
enables it to exrat features from the Input Image. This results in a tensor
called feature map with a shape of T/4 H/16 W/16. The smaller shape is caused by
the convolutional layers of the CNN with the filter and kernels.
This feature map his here represented by the blue cube and the centeral
key-frame is the yeallow slide.



9. RPN  TODO : should I explain how RPN workds

Will take this key center frame as input to generate s multiple potential person
bounding boxes along with objectness scores.

How many of this bounding box proposals are gernerate are controled with an
hyperparameter called R which is here 300. All of those proposals  
have differtent sizes so they need to be passed through an RoIPool before this
Tensor can be feed to the Action Transformer head since there must have the
same size.


10. Action Transformer Head 

Now we are coming to the main contribution of this work the Action Transformer
Head. This is a Transformer Network for Images inspiered from the Transformer
network for sequence to sequence task which Prof. Ronneberger explained in the
lecture before Christmas. It has the same main structure it has a Query and
Memory. The memory is spilt in 2 parts. A Key tensor and a value tensor. 

As you can see here this Action Transformer head here refered as Tx Unit.
Can be stacked and have different layers in the Experiments we will Analyse the
optimal combination of heads.


___________________________More Detailed version _______________________________
(based on the section 3.4 Implementation Details

13. I3D base highlighted 

This I3D base is a CNN pre-trained on the data set Kinetic-400. The filters of
the different layer are already able to extract meaningfull features to
recoginze object in a Image. This Operation will Downsample our Input tensor.


14. Convolution feature is highlighted

The Input of size 64x400x400 will result in a Convolutional feature map of size
16x25x25. This blue cube repesents context frames around the key frame which is
highlighted in yellow.



They take the center of the feature map and add a location embedding. This
allows the model to encode spatiotemporal proximity. Which is importen without
this this proberty would have been lost when we removed the RNN from the model.


15. RPN 

This key frame embedding of the feature map will be the query from the
attention. It will be passed trough a region proposal network which will
generate box proposal for the people in side the key frame. How many of them 
is a hyperparameter called R and is at default at 300. We will see later in the
Expriments that by changing the amount it will increase or decrease the
accurency of the model.

16. Tx  Unit














14.Tx Unit 


Here we can see that Tx Unit has two stream. The ROI Pooling extract  









14.





















27.Slide Expriments

Now we are going to evaluate the model.
The dataset he autor used  is the Atomic Visual Actions (AVA) dataset.


this dataset conists of 211 thousend training clips 57 thousend for valdiation
and 117 thousend for testing each clip is 15 minits long TODO ?

For the center frame the labels are bounding boxes for the persons and the
action of that person. 


For the 








30.Slide Action classication with and without GT box

To isolate the classification task from the localization task
we compare the first case the action classication with and
without groundtrue boxes for different model configurations.
The results are messuerd with 64 proposal of the RPN.


In the 2 first rows we have the I3D as a trunk and Head  .
We can opserve that the param stays the same as  compuation cost like we
would accpect since we only turn the groundtrue labels on and off. 

But  ground truelabels help the model to get a 2 % better mAP on the validation set.
Which indicates that the RPN propels are not perfect and correct bounding boxes
help the I3D head improve the action classication.


In the last 4 bottom rows the Head is replaced with the proposed Action Transformer Head.
They also compared the performence with Low resolutions and high resolutions queries.


In the coluom for the model parameter we see that the higher queries resolution
increase the parameters  which leades to a higher compute cost but not
linearaly.

How does this affect our performence?

While without GT boxes the mAP stays with  18 or 19 % lower than  with the I3D head 
the performence with GT boxes is with 29 % for low resolution and for high
resolution 27.6% better.

An explanation for this could be the RPN proposals are not as good as the ground
true. Since the action transformer uses this areas to extrat the data for this keys and values.

TODO: what could be the reson of this?


31. Slide Case 2 localization performence


here we have the second case we evaluate the action transformer head
localization performence compared to the I3D head. 
As we can see the I3D has a higher accurency in both cases
which leads from his global perspective. 
When comparing the low resolution and high query preprocessor the action
transformer head has a significant better performence when using the high
resolutions quiers.






32. Performence deppending on numbers of head and layers 

Now we evaluate the performence of the action transformer by varing the amount
of layers and heads. This values are obtained by using groundtrue box as proposal.
Those indicate the performence increase by more layers and using less heads.

TODO what could be the reason for this?



33. Compare to state of the Art


In this table the proposed Video Action Transformer is compared to other state
of the art models 




34. Correlation of  performence with training data size


In this graph we can see the Correlation between the training data size and the
mAp of the network.

The green line indicates the training size and it grows from left to right and
from the smallest set from around 500 samples with people pointing to an
object. to over 200.000 samples of people just standing.
The blue and red bars are an comparison between using the I3D as a head and the
action transformer as a head
The blue bars are represent the mAP performence in case of the I3D  and red is
for the action transformer. 

It seems that a lot of action benefit of more data but also a some  like
touching an object or smoke have still a poor performence compare to other tasks.

The swimming task are prediced quite well with less data probably because the
model learns if there is water (so the most of the image is blue) it means the
action is swimming .

Its also a good example where the action transformer head is supiror then the I3D head
also drving, smoking,  hand claps and other action are benfit from that the
action transformer attent to the hands and head of a person.

TODO: kissing is worse even it has 







35 Examples of Correct Predictions 

The autors present in their work this images where the model was able to
predict the action and person correctly. There are 3 Image each for the task
holding  an object, fight and watch a person. In all of them the model needs to
reason about the surounding of the person and over time 



36 Example of Incorrect Predictions 


We saw earlier that the model is performening poorly on reconizing smoking
people which is not a trival task. Here we can see why.

In the first picture a men is eaten something so he puts his hand to his mouth,
which causes the network to predict him as smoking even he is not.

Secend Picture the false person is marked as smoking 


Third TODO?


In the bottom left Image the person is actually singing , the model is triggered
because he has a microphone in front of his mouth 

A particualar interessing one is this guy how is smoking in front of a mirrow 
so the model is 











































The video action transformer

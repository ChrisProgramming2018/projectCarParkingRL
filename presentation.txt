





Hello My name is Christian Leininger and I am going to present you you the Video
Action Transformer a work Girdhar el al.

The objective of this work is to localize all humans and their associated action in video clips

To achive this the author propose this model called "Video action transformer",
which takes a raw pixel image as input.


One reason why inferring the actions of the person is 
challaneging. is that the model needs to  able to
capture the information over time (temporal) and contextual information (other
people, other objects). 
Example action are "watching a person ", "listen to a person", "pointing to an
object" and many more. 
So far RNN have been used for this task. Professor Ronneberger already explained
in the lecture before Chrismas that one issue of this method is that the
information needs to be processd sequenicely which increase the compute time.
The serious issue is that in the RNN method the information collected over time
is saved in one vector. This leads to to information lost when the input
sequence is too long. Like it your case. To overcame this issue the authors
propose this Video Action Transformer architecure that uses a techique called attention.
This is inspiered by the The Transformer architecture from Vaswani et al. which
has been successful apllied to  sequence modelling tasks like language
translation. The attention method I will explain later in more detail, but one
drawback is that the compute cost for the basic attention method is O(n2) where
n is the amout of pixel of the inputImage. A major contribution of this work is
that the Video Action Transformer does not requiere so much computational time



3.Slide Image 


This Image is from the original paper and should illustrate what the model is
learning. 

On the right Image is demonstarted how the output of the model looks like.
We can see that the model creates an bounding box for each person it is
reconizing and what is the action with the highest probabilty azosatied to this
person.


In the bottom left image it is illustradet the area where the model is
attenting to the heands and faces.










27.Slide 

Now are we coming to the part where we are going t evaluate the model the autor
used the Atomic Visual Actions (AVA) dataset

this dataset conists of 211 thousend training clips 57 thousend for valdiation
and 117 thousend for testing each clip is 15 minits long TODO ?

For the center frame the labels are bounding boxes for the persons and the
action of that person. 


For the 








30.Slide Action classication 


In this table we compare the action classication perscent  of different model
configuration.

In the 2 first rows we have the I3D as a trunk and Head  .
We can opserve that the param stays the same so does the compuation cost like we
would accpect since we only turn the ground truelabels on and off. 

But  ground truelabels help the model to get a 2 % better Val mAP.
Which indicates that the RPN propels are not perfect.

In the bottem 4 rows the Head is replaced with the proposed Action Transformer Head.
They also compared the performence with Low resolutions and high resolutions
resolutions queries.


In the coluom for the model parameter we see that the higher  queries resolution
increase the parameters  which leades  to a higher compute cost but not
linearaly.

How does this affect our performence?

While without GT boxes the mAP stays with  18 or 19 % lower than  with the I3D head 
the performence with GT boxes is with 29 %  for low resolution and for high
resolution 27.6% better.

TODO: what could be the reson of this?



This indicates that the Transformer head depends on good box propels for good action
classication. Since could come from that the action Transformer attents to this
region.












The video action transformer

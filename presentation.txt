





Hello My name is Christian Leininger and I am going to present you you the Video
Action Transformer a work Girdhar el al.

The objective of this work is to localize all humans and their associated action in video clips

To achive this the author propose this model called "Video action transformer",
which takes a raw pixel image as input.


One reason why inferring the actions of the person is 
challaneging. is that the model needs to  able to
capture the information over time (temporal) and contextual information (other
people, other objects). 
Example action are "watching a person ", "listen to a person", "pointing to an
object" and many more. 
So far RNN have been used for this task. Professor Ronneberger already explained
in the lecture before Chrismas that one issue of this method is that the
information needs to be processd sequenicely which increase the compute time.
The serious issue is that in the RNN method the information collected over time
is saved in one vector. This leads to to information lost when the input
sequence is too long. Like it your case. To overcame this issue the authors
propose this Video Action Transformer architecure that uses a techique called attention.
This is inspiered by the The Transformer architecture from Vaswani et al. which
has been successful apllied to  sequence modelling tasks like language
translation. The attention method I will explain later in more detail, but one
drawback is that the compute cost for the basic attention method is O(n2) where
n is the amout of pixel of the inputImage. A major contribution of this work is
that the Video Action Transformer does not requiere so much computational time



3.Slide Image 


This Image is from the original paper and should illustrate what the model is
learning. 

On the right Image is demonstarted how the output of the model looks like.
We can see that the model creates an bounding box for each person it is
reconizing and what is the action with the highest probabilty azosatied to this
person.


In the bottom left image it is illustradet the area where the model is
attenting to the heands and faces.










27.Slide Expriments

Now we are going to evaluate the model.
The dataset he autor used  is the Atomic Visual Actions (AVA) dataset.


this dataset conists of 211 thousend training clips 57 thousend for valdiation
and 117 thousend for testing each clip is 15 minits long TODO ?

For the center frame the labels are bounding boxes for the persons and the
action of that person. 


For the 








30.Slide Action classication with and without GT box

To isolate the classification task from the localization task
we compare the first case the action classication with and
without groundtrue boxes for different model configurations.
The results are messuerd with 64 proposal of the RPN.


In the 2 first rows we have the I3D as a trunk and Head  .
We can opserve that the param stays the same as  compuation cost like we
would accpect since we only turn the groundtrue labels on and off. 

But  ground truelabels help the model to get a 2 % better mAP on the validation set.
Which indicates that the RPN propels are not perfect and correct bounding boxes
help the I3D head improve the action classication.


In the last 4 bottom rows the Head is replaced with the proposed Action Transformer Head.
They also compared the performence with Low resolutions and high resolutions queries.


In the coluom for the model parameter we see that the higher queries resolution
increase the parameters  which leades to a higher compute cost but not
linearaly.

How does this affect our performence?

While without GT boxes the mAP stays with  18 or 19 % lower than  with the I3D head 
the performence with GT boxes is with 29 % for low resolution and for high
resolution 27.6% better.

An explanation for this could be the RPN proposals are not as good as the ground
true. Since the action transformer uses this areas to extrat the data for this keys and values.

TODO: what could be the reson of this?


31. Slide Case 2 localization performence


here we have the second case we evaluate the action transformer head
localization performence compared to the I3D head. 
As we can see the I3D has a higher accurency in both cases
which leads from his global perspective. 
When comparing the low resolution and high query preprocessor the action
transformer head has a significant better performence when using the high
resolutions quiers.






32. Performence deppending on numbers of head and layers 

Now we evaluate the performence of the action transformer by varing the amount
of layers and heads. This values are obtained by using groundtrue box as proposal.
Those indicate the performence increase by more layers and using less heads.

TODO what could be the reason for this?



33. Compare to state of the Art


In this table the proposed Video Action Transformer is compared to other state
of the art models 





















































The video action transformer

Hello 
My name is Christian Leininger, and I am going to present you the work Girdhar el al. called "Video Action Transformer Network."

The objective of this work is to localize all humans and their associated actions in video clips.

To achieve this, the authors propose a model called "Video action transformer" , 
which able to do this by only taking raw pixel images as input. 
Other model uses flow estimation or sound to predict the action.





Inferring the actions of people in video clips is a challenging task.
One reason for this is that the model needs to able to
capture the information over time (temporal) and contextual information (other people, other objects). 
Some Example actions which explain this very well are 
"watching a person", "listen to a person", "pointing to an
object" and many more. 
 
So far, RNN have been used for this task. Professor Ronneberger already explained in the lecture before Chrismas.
That one drawback of this method is that the
information is processed in sequence, which increases the compute time.
However, a more serious drawback is that in the RNN method, the information collected over time is saved in one vector. 
This leads to a loss of information when the input
sequence is too long. 

Like in this case . To overcome this, the authors propose this Video Action Transformer architecture 
that uses a technique called attention.
This is inspired by  The Transformer architecture from Vaswani et al., 
which has been successfully applied to sequence modeling tasks like language translation. 
The attention method I will explain later in more detail, but one drawback is that the compute cost for 
the basic attention method is O(n2) where
n is the amount of pixel of the input image. The main contribution of this work is that the Video Action Transformer, 
which can be applied to video clips and needs less computation time. 




3.Slide Image 


This Image from the original paper should illustrate what the model is learning. 

The right Image demonstrated how the model is adding bounding boxes and the action with the highest probability to the people in the Image.

In the bottom left Image, it is illustrated the area of the average softmax attention corresponding to the person in the red box.
In the top left Image, show their ‘key’ embeddings as color-coded 3D PCA projection for two of the six heads in our 2-head 3-layer Tx head.

Abstract 
In this work, the authors proposed the Action Transformer model to localize all humans in a video clip  
and detect their actions. The model  outperforms other state of the art models at this challenging task by 
using a technique called self-attention. This enables the network to emphasize at hands and faces, which 
are often crucial to discriminate an action.




The Video Action Transformer model consists of 2 parts a trunk(base) and head.
The trunk takes raw RGB frame a so called t-frame as input. That is the key frame and 3 sec encoded
around it. The layer weights Conovulal Neural network in the trunk are Initlised
by a pre trained model. This enables the CNN to extract meaningfull data from
the input. From this feature map the location embedding of the key frame is used
from the RPN to create bounding box proposals. Each of those proposals could
have a different shape so they need to processed by a RoIPool Operation to get
the same shape. Because the Input for the Action Transformer Network needs to
have a spesific shape this feature Tensor is called Query. It should contain
information about the person being classified 
